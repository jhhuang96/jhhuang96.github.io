<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>数据预处理</title>
    <url>/2020/03/31/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<ol type="1">
<li><p>在获取的数据中经常会遇到唯一属性，这些属性通常是人为添加的一些id属性，如存放在数据库中的自增的主键。</p>
<p>对于这些属性，它并不能刻画样本自身的分布规律。所以只需要简单的删除这些属性。</p></li>
<li><p>对于数据中的某个属性，如果其方差很小，则意味着其识别能力较弱。极端情况下其方差为0，这意味着该属性在所有样本上的值都是恒定的。</p>
<p>因此可以设定一个阈值（如<span class="math inline">\(10^{-3}\)</span> ），将方差小于该阈值的属性直接剔除。</p></li>
</ol>
<h2 id="一缺失值处理">一、缺失值处理</h2>
<p>缺失值的处理有三种方法：</p>
<ul>
<li><p><strong>直接使用</strong>含有缺失值的数据。</p>
<p>某些算法可以直接使用含有缺失值的情况，如<strong>决策树</strong>算法可以直接使用含有缺失值的数据。</p>
<ul>
<li>优点：直接使用原始数据，排除了人工处理缺失值带来的信息损失。</li>
<li>缺点：只有少量的算法支持这种方式。</li>
</ul></li>
<li><p><strong>删除</strong>含有缺失值的数据。</p>
<p>最简单的办法就是删除含有缺失值的样本。</p>
<ul>
<li><p>优点：简单、高效。</p></li>
<li><p>缺点：如果样本中的缺失值较少，则直接丢弃样本会损失大量的有效信息。这是对信息的极大浪费。</p>
<blockquote>
<p>如果样本中包含大量的缺失值，只有少量的有效值，则该方法可以接受。</p>
</blockquote></li>
</ul></li>
<li><p>缺失值<strong>补全</strong>。</p>
<p>用最可能的值来<strong>插补</strong>缺失值。这也是在实际工程中应用最广泛的技术。</p>
<ul>
<li>优点：保留了原始数据</li>
<li>缺点：计算复杂，而且当插补的值估计不准确时，会对后续的模型引入额外的误差。</li>
</ul></li>
</ul>
<p>缺失值补全常见有以下方法：</p>
<ul>
<li><p><strong>均值插补</strong></p></li>
<li><p>同类均值插补</p></li>
<li><p>建模预测</p></li>
<li><p>高维映射</p></li>
<li><p>多重插补</p></li>
<li><p>压缩感知及矩阵补全</p></li>
</ul>
<h3 id="均值插补-同类均值插补">1.1 均值插补 &amp; 同类均值插补</h3>
<ol type="1">
<li><p>均值插补：</p>
<ul>
<li>如果样本的属性是<strong>连续值</strong>，则该属性的缺失值就以该属性有效值的<strong>平均值</strong>来插补。</li>
<li>如果样本的属性是<strong>离散值</strong>，则该属性的缺失值就以该属性有效值的<strong>众数</strong>（出现频率最高的值）来插补。</li>
</ul></li>
<li><p>均值插补在含有缺失值的属性上的所有缺失值都填补为同一个值。</p>
<p>而<strong>同类均值插补</strong>首先将样本进行<strong>分类</strong>，然后以<strong>该类</strong>中的样本的均值来插补缺失值。</p></li>
</ol>
<h3 id="建模预测">1.2 建模预测</h3>
<ol type="1">
<li><p>建模预测的思想是：将缺失的属性作为预测目标，通过建立模型来预测。</p></li>
<li><p>给定数据集<span class="math inline">\(\mathbb{D}=\left\{\left(\overrightarrow{\mathbf{x} }_{1}, \tilde{y}_{1}\right),\left(\overrightarrow{\mathbf{x} }_{2}, \tilde{y}_{2}\right), \cdots,\left(\overrightarrow{\mathbf{x} }_{N}, \tilde{y}_{N}\right)\right\}\)</span> 。</p>
<p>假设属性<span class="math inline">\(j\)</span>含有缺失值，根据<span class="math inline">\(x_{i,j}\)</span>是否缺失，将数据集划分为：</p>
<ul>
<li><span class="math inline">\(\mathbb{D}_{1}=\left\{\overrightarrow{\mathbf{x} }_{i} | x_{i, j} \neq n u l l\right\}\)</span> : 属性 <span class="math inline">\(j\)</span> <strong>有效</strong>的样本的集合。</li>
<li><span class="math inline">\(\mathbb{D}_{2}=\left\{\overrightarrow{\mathbf{x} }_{i} | x_{i, j} = n u l l\right\}\)</span> : 属性 <span class="math inline">\(j\)</span> <strong>缺失</strong>的样本的集合。</li>
</ul>
<p>将 <span class="math inline">\(\mathbb{D}_{1}\)</span> 中的样本作为新的训练集，标签值重新定义为属性 <span class="math inline">\(j\)</span> 的值，通过建模来完成属性 <span class="math inline">\(j\)</span> 的学习。将 <span class="math inline">\(\mathbb{D}_{2}\)</span> 中的样本作为测试集，通过学得的模型来预测其属性 <span class="math inline">\(j\)</span> 的值。</p></li>
<li><p>这种方法的效果相对较好，但是该方法有个根本缺陷：</p>
<ul>
<li>如果其他属性和属性 <span class="math inline">\(j\)</span> 无关，则预测的结果无意义。</li>
<li>如果预测结果相当准确，则又说明属性 <span class="math inline">\(j\)</span> 可以由其它属性计算得到， 于是属性 <span class="math inline">\(j\)</span> 信息冗余，没有必要纳入数据集中。</li>
</ul>
<p>一般的情况是介于两者之间。</p></li>
</ol>
<h3 id="高维映射">1.3 高维映射</h3>
<ol type="1">
<li>高维映射的思想是：将属性映射到高维空间。</li>
<li>给定数据集<span class="math inline">\(\mathbb{D}\)</span>，假设属性 <span class="math inline">\(j\)</span> 的取值为离散值<span class="math inline">\({a_1,a_2,...,a_K}\)</span>一共<span class="math inline">\(K\)</span>个值，则将该属性扩展为<span class="math inline">\(K+1\)</span>个属性<span class="math inline">\((j_1,j_2,...,j_{K+1})\)</span>，其中：
<ul>
<li>若样本在属性 <span class="math inline">\(j\)</span> 上的取值为<span class="math inline">\(a_k\)</span>，则样本在新的属性<span class="math inline">\(j_k\)</span>上的取值为 1，在新的属性<span class="math inline">\(j_1,...,j_{k-1},j_{k+1},...,j_{K+1}\)</span>上的取值为 0 。</li>
<li>若样本在属性 <span class="math inline">\(j\)</span> 上缺失，则样本的新的属性<span class="math inline">\(J_{K+1}\)</span>上的取值为 1,在新的属性<span class="math inline">\(j_1,...,j_K\)</span>上取值为 0 。</li>
</ul></li>
<li>对于连续特征，高维映射无法直接处理。可以在连续特征离散化之后，再进行高维映射。</li>
<li>高维映射是最精确的做法，它完全保留了所有的信息，也未增加任何额外的信息。比如广告的<code>CTR</code>预估模型，预处理时会把所有变量都这样处理，达到几亿维。
<ul>
<li>优点：完整保留了原始数据的全部信息。</li>
<li>缺点：计算量大大提升。而且只有在样本量非常大的时候效果才好，否则会因为过于稀疏，效果很差。</li>
</ul></li>
</ol>
<h2 id="二特征编码">二、特征编码</h2>
<h3 id="特征二元化">2.1. 特征二元化</h3>
<ol type="1">
<li>特征二元化的过程是将<strong>数值型</strong>的属性转换为<strong>布尔值</strong>的属性。通常用于假设属性取值为取值分布为<strong>伯努利分布</strong>的情形。</li>
<li>特征二元化的算法比较简单。 对属性<span class="math inline">\(j\)</span>指定一个阈值<span class="math inline">\(\epsilon\)</span>。
<ul>
<li>如果样本在属性 <span class="math inline">\(j\)</span> 上的值大于等于 <span class="math inline">\(\epsilon\)</span> ，则二元化之后为 1 。</li>
<li>如果样本在属性 <span class="math inline">\(j\)</span> 上的值小于 <span class="math inline">\(\epsilon\)</span> ，则二元化之后为 0 。</li>
</ul></li>
<li>阈值 <span class="math inline">\(\epsilon\)</span> 是一个超参数，其选取需要结合模型和具体的任务来选择。</li>
</ol>
<h3 id="one-hot">2.2. one-hot</h3>
<ol type="1">
<li><p>对于非数值属性，如<code>性别：[男，女]、国籍：[中国，美国，英国]</code>等等，可以构建一个到整数的映射。如<code>性别：[男，女]</code>属性中，将<code>男</code>映射为整数 1、<code>女</code>映射为整数 0。</p>
<p>该方法的优点是简单。但是问题是，在这种处理方式中无序的属性被看成有序的。<code>男</code>和<code>女</code>无法比较大小，但是<code>1</code>和<code>0</code>有大小。</p>
<p>解决的办法是采用独热码编码<code>One-Hot Encoding</code>。</p></li>
<li><p><code>One-Hot Encoding</code>采用<code>N</code>位状态位来对<code>N</code>个可能的取值进行编码，每个取值都由独立的状态位来表示，并且在任意时刻只有其中的一位有效。</p></li>
<li><p><code>One-Hot Encoding</code> 的优点：</p>
<ul>
<li>能够处理非数值属性。</li>
<li>在一定程度上也<strong>扩充</strong>了特征。如<code>性别</code>是一个属性，经过独热码编码之后变成了<code>是否男</code> 和 <code>是否女</code> 两个属性。</li>
<li>编码后的属性是<strong>稀疏</strong>的，存在大量的零元分量。</li>
</ul>
<p>缺点：当类别的数量很多时，特征空间会变得非常大。在这种情况下，一般可以用主成分分析(PCA)来<strong>减少维度</strong>。而且<strong>one-hot encoding + PCA</strong>这种组合在实际中也非常有用。</p></li>
<li><p>在决策树模型中，并不推荐对离散特征进行<code>one-hot</code>。 主要有两个原因：</p>
<ul>
<li><p>产生样本切分不平衡的问题，此时切分增益会非常小。</p>
<p>如：<code>国籍</code>这个离散特征经过独热码编码之后，会产生<code>是否中国、是否美国、是否英国、...</code> 等一系列特征。在这一系列特征上，只有少量样本为<code>1</code>，大量样本为<code>0</code>。</p>
<p>这种划分的<strong>增益</strong>非常小，因为拆分之后：</p>
<ul>
<li>较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略。</li>
<li>较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。</li>
</ul></li>
<li><p>影响决策树的学习。</p>
<p>决策树依赖的是数据的统计信息。而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上，统计信息是不准确的，学习效果变差。</p></li>
</ul></li>
</ol>
<h3 id="离散化">2.3 离散化</h3>
<ol type="1">
<li>离散化用于将连续的数值属性转化为离散的数值属性。</li>
<li>是否使用特征离散化，这背后是：使用“海量离散特征+简单模型”，还是“少量连续特征+复杂模型”。
<ul>
<li>对于线性模型，通常使用“海量离散特征+简单模型”。
<ul>
<li>优点：模型简单。</li>
<li>缺点：特征工程比较困难。但是一旦有成功的经验就可以推广，并且可以很多人并行研究。</li>
</ul></li>
<li>对于非线性模型（如深度学习），通常使用“少量连续特征+复杂模型”。
<ul>
<li>优点是：不需要进行复杂的特征工程。</li>
<li>缺点是：模型复杂。</li>
</ul></li>
</ul></li>
</ol>
<h4 id="特性">2.3.2 特性</h4>
<ol type="1">
<li><p>在工业界很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列 0/1 的离散特征。</p>
<p>其优势有：</p>
<ul>
<li><p>离散化之后得到的稀疏向量，内积乘法运算<strong>速度更快</strong>，计算结果方便存储。</p></li>
<li><p>离散化之后的特征对于<strong>异常数据</strong>具有很强的<strong>鲁棒性</strong>。</p>
<p>如：销售额作为特征，当销售额在 <code>[30,100)</code> 之间时，为1，否则为 0。如果未离散化，则一个异常值 10000 会给模型造成很大的干扰。由于其数值较大，它对权重的学习影响较大。</p></li>
<li><p>逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于引入了非线性（分段线性映射），<strong>提升模型的表达能力</strong>，增强拟合能力。</p></li>
<li><p>离散化之后可以进行<strong>特征交叉</strong>。假设有连续特征 <span class="math inline">\(j\)</span> ，离散化为 <span class="math inline">\(N\)</span> 个 0/1 特征；连续特征 <span class="math inline">\(k\)</span> ，离散化为 <span class="math inline">\(M\)</span> 个 0/1 特征，则分别进行离散化之后引入了 <span class="math inline">\(M+N\)</span> 个特征。</p>
<p>假设离散化时，并不是独立进行离散化，而是特征 <span class="math inline">\(j,k\)</span> 联合进行离散化，则可以得到<span class="math inline">\(M*N\)</span>个组合特征。这会进一步引入非线性，<strong>提高模型表达能力</strong>。</p></li>
<li><p>离散化之后，模型会更<strong>稳定</strong>。</p>
<p>如对销售额进行离散化，<code>[30,100)</code> 作为一个区间。当销售额在40左右浮动时，并不会影响它离散化后的特征的值。</p>
<p>但是处于区间连接处的值要小心处理，另外如何划分区间也是需要仔细处理。</p></li>
</ul></li>
<li><p>特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险。</p>
<p>能够对抗过拟合的原因：经过特征离散化之后，模型<strong>不再拟合特征的具体值</strong>，而是<strong>拟合特征的某个概念</strong>。因此能够对抗数据的扰动，更具有鲁棒性。</p>
<p>另外它使得模型要拟合的值大幅度降低，也<strong>降低了模型的复杂度</strong>。</p></li>
</ol>
<h2 id="三数据标准化正则化">三、数据标准化、正则化</h2>
<h3 id="数据标准化">3.1. 数据标准化</h3>
<ol type="1">
<li><p>数据标准化是将样本的属性取值缩放到某个指定的范围。</p></li>
<li><p>数据标准化的两个原因：</p>
<ul>
<li><p>某些算法要求样本数据的属性取值具有零均值和单位方差。</p></li>
<li><p>样本不同属性具有不同量级时，<strong>消除数量级</strong>的影响。如下图所示为两个属性的目标函数的等高线。</p>
<ul>
<li><p>数量级的差异将导致量级较大的属性占据主导地位。</p>
<p>从图中看到：如果样本的某个属性的量级特别巨大，将原本为椭圆的等高线压缩成直线，从而使得目标函数值仅依赖于该属性。</p></li>
<li><p>数量级的差异将导致迭代<strong>收敛速度减慢</strong>。</p>
<p>原始的特征进行梯度下降时，每一步梯度的方向会<strong>偏离</strong>最小值（等高线中心点）的方向，迭代次数较多，且学习率必须非常小，否则非常容易引起宽幅震荡。</p></li>
</ul>
<p>标准化后进行梯度下降时，每一步梯度的方向都几乎<strong>指向</strong>最小值（等高线中心点）的方向，迭代次数较少。</p>
<ul>
<li><p>所有<strong>依赖于样本距离</strong>的算法对于数据的数量级都非常<strong>敏感</strong>。</p>
<p>如<span class="math inline">\(k\)</span>近邻算法需要计算距离当前样本最近的<span class="math inline">\(k\)</span>个样本。当属性的量级不同时，选取的最近的 <span class="math inline">\(k\)</span>个样本也会不同。</p></li>
</ul></li>
</ul>
<figure>
<img src="/2020/03/31/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/standardization.jpg" alt><figcaption>standardization</figcaption>
</figure></li>
<li><p>设数据集 <span class="math inline">\(D=\left\{\left(\overrightarrow{\mathbf{x} }_{1}, \tilde{y}_{1}\right),\left(\overrightarrow{\mathbf{x} }_{2}, \tilde{y}_{2}\right), \cdots,\left(\overrightarrow{\mathbf{x} }_{N}, \tilde{y}_{N}\right)\right\}, \overrightarrow{\mathbf{x} }_{i}=\left(x_{i, 1}, \cdots, x_{i, n}\right)^{T}\)</span> 。常用的标准化算法有：</p>
<ul>
<li><p><code>min-max</code>标准化：对于属性 ，设所有样本在属性 上的最大值为 ，最小值为 。则标准化后的属性值为： <span class="math display">\[
\hat{x}_{i, j}=\frac{x_{i, j}-j_{\min } }{j_{\max }-j_{\min } }
\]</span></p>
<p>标准化之后，所有样本在属性 上的取值都在 <code>[0,1]</code> 之间。</p></li>
<li><p><code>z-score</code>标准化：对于属性 <span class="math inline">\(j\)</span> ，设所有样本在属性 <span class="math inline">\(j\)</span> 上的均值为 <span class="math inline">\(\mu_j\)</span> ，方差为 <span class="math inline">\(\sigma_j\)</span> 。则标准化后的属性值为： <span class="math display">\[
\hat{x}_{i, j}=\frac{x_{i, j}-\mu_j}{\sigma_j}
\]</span></p>
<p>标准化之后，样本集的所有属性的均值都为 0，标准差均为 1。</p></li>
</ul></li>
<li><p>注意：如果数据集分为训练集、验证集和测试集，则：训练集、验证集、测试集使用相同标准化参数，该参数的值都是从<strong>训练集</strong>中得到。</p>
<ul>
<li>如果使用<code>min-max</code> 标准化，则属性 <span class="math inline">\(j\)</span> 的标准化参数<span class="math inline">\(j_{max},j_{min}\)</span>,都是从训练集中计算得到。</li>
<li>如果使用<code>z-score</code> 标准化，则属性<span class="math inline">\(j\)</span>的标准化参数<span class="math inline">\(\mu_j, \sigma_j\)</span>都是从训练集中计算得到。</li>
</ul></li>
</ol>
<h2 id="四特征选择">四、特征选择</h2>
<ol type="1">
<li><p>对于一个学习任务，给定了属性集，其中某些属性可能对于学习来说是很关键的，但是有些属性可能就意义不大。</p>
<ul>
<li><p>对当前学习任务有用的属性称作相关特征<code>relevant feature</code> 。</p></li>
<li><p>对当前学习任务没有用的属性称作无关特征<code>irrelevant feature</code> 。</p></li>
</ul>
<p>从给定的特征集合中选出相关特征子集的过程称作特征选择<code>feature selection</code>。</p></li>
<li><p>特征选择可能会降低模型的预测能力。因为被剔除的特征中可能包含了有效的信息，抛弃了这部分信息会一定程度上降低预测准确率。</p>
<p>这是计算复杂度和预测能力之间的折衷：</p>
<ul>
<li><p>如果保留尽可能多的特征，则模型的预测能力会有所提升，但是计算复杂度会上升。</p></li>
<li><p>如果剔除尽可能多的特征，则模型的预测能力会有所下降，但是计算复杂度会下降。</p></li>
</ul></li>
<li><p>常见的特征选择方法大致分为三类：过滤式<code>filter</code>、包裹式<code>wrapper</code>、嵌入式<code>embedding</code>。</p></li>
</ol>
<h3 id="嵌入式选择">4.1 嵌入式选择</h3>
<p>过滤方法与机器学习算法相互独立，而且不需要交叉验证，计算效率比较高，但是没有考虑机器学习算法的特点。包裹方法使用预先定义好的机器学习算法来评估特征子集的质量，需要很多次训练模型，计算效率很低。嵌入方法则将特征选择嵌入到模型的构建过程中，具有包裹方法与机器学习算法相结合的优点，而且具有过滤方法计算效率高的优点，嵌入式方法是实际应用中最常见的方法，弥补了前面两种方法的不足。</p>
<ol type="1">
<li><p>以线性回归模型为例。</p>
<p>给定数据集<span class="math inline">\(D=\left\{\left(\overrightarrow{\mathbf{x} }_{1}, \tilde{y}_{1}\right),\left(\overrightarrow{\mathbf{x} }_{2}, \tilde{y}_{2}\right), \cdots,\left(\overrightarrow{\mathbf{x} }_{N}, \tilde{y}_{N}\right)\right\}, \tilde{y_i} \in \mathbb{R}\)</span> 。 以平方误差为损失函数，则优化目标为： <span class="math display">\[
\min _{\overrightarrow{\mathbf{w} }} \sum_{i=1}^{N}\left(\tilde{y}_{i}-\overrightarrow{\mathbf{w} }^{T} \overrightarrow{\mathbf{x} }_{i}\right)^{2}
\]</span></p>
<ul>
<li><p>如果使用<span class="math inline">\(L_2\)</span>范数正则化，则优化目标为： <span class="math display">\[
\min _{\overrightarrow{\mathbf{w} }} \sum_{i=1}^{N}\left(\tilde{y}_{i}-\overrightarrow{\mathbf{w} }^{T} \overrightarrow{\mathbf{x} }_{i}\right)^{2}+\lambda\|\overrightarrow{\mathbf{w} }\|_{2}^{2}, \quad \lambda&gt;0
\]</span> 此时称作岭回归<code>ridge regression</code> 。</p></li>
<li><p>如果使用<span class="math inline">\(L_1\)</span>范数正则化，则优化目标为： <span class="math display">\[
\min _{\overrightarrow{\mathbf{w} }} \sum_{i=1}^{N}\left(\tilde{y}_{i}-\overrightarrow{\mathbf{w} }^{T} \overrightarrow{\mathbf{x} }_{i}\right)^{2}+\lambda\|\overrightarrow{\mathbf{w} }\|_{1}, \quad \lambda&gt;0
\]</span> 此时称作<code>LASSO:Least Absolute Shrinkage and Selection Operator</code> 回归。</p></li>
</ul></li>
<li><p>引入<span class="math inline">\(L_1\)</span>范数除了<strong>降低过拟合风险</strong>之外，还有一个好处：它求得的<span class="math inline">\(\overrightarrow{\mathbf{w} }\)</span>会有较多的分量为零。即：它更容易获得<strong>稀疏解</strong>。</p>
<p>于是基于 <span class="math inline">\(L_1\)</span> 正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，二者同时完成。</p></li>
<li><p>常见的嵌入式选择模型：</p>
<ul>
<li><p>在<code>Lasso</code>中，<span class="math inline">\(\lambda\)</span> 参数控制了稀疏性：</p>
<ul>
<li><p>如果<span class="math inline">\(\lambda\)</span>越小，则稀疏性越小，则被选择的特征越多。</p></li>
<li><p>如果<span class="math inline">\(\lambda\)</span>越大，则稀疏性越大，则被选择的特征越少。</p></li>
</ul></li>
<li><p>在<code>SVM</code>和<code>logistic-regression</code>中，参数<code>C</code>控制了稀疏性</p>
<ul>
<li><p>如果<code>C</code>越小，则稀疏性越大，则被选择的特征越少。</p></li>
<li><p>如果<code>C</code>越大，则稀疏性越小，则被选择的特征越多。</p></li>
</ul></li>
</ul></li>
<li><p>另外一种嵌入方法是基于<strong>树模型</strong>的特征选择方法。在决策树中，深度较浅的节点一般对应的特征分类能力更强（可以将更多的样本区分开）。对于基于决策树的算法，如随机森林，重要的特征更有可能出现在深度较浅的节点，而且出现的次数可能越多。因此，可以基于树模型中特征出现次数等指标对特征进行<strong>重要性排序</strong>。</p></li>
</ol>
<h2 id="五多类分类问题">五、多类分类问题</h2>
<ol type="1">
<li><p>某些算法原生的支持多分类，如：决策树、最近邻算法等。但是有些算法只能求解二分类问题，如：支持向量机。</p></li>
<li><p>对于只能求解二分类问题的算法，一旦遇到问题是多类别的，那么可以将多分类问题<strong>拆解成二分类任务</strong>求解。</p>
<p>即：</p>
<ul>
<li><p>先对原问题进行拆分，然后为拆出的每个二分类任务训练一个分类器。</p></li>
<li><p>测试时，对这些二分类器的预测结果进行集成，从而获得最终的多分类结果。</p></li>
</ul></li>
<li><p>多分类问题有三种拆解方式：</p>
<ul>
<li><p>一对其余(<code>One-vs-rest:OvR</code>) 。</p></li>
<li><p>一对一(<code>one-vs-one:OvO</code>) 。</p></li>
<li><p>多对多(<code>many-vs-many:MvM</code>) 。</p></li>
</ul></li>
</ol>
<h3 id="one-vs-rest">5.1 one vs rest</h3>
<ol type="1">
<li><p><strong>一对其余</strong>：为<strong>每一个类别</strong>训练一个分类器。</p>
<p>假设类别为<span class="math inline">\(\{c_1,c_2,..,c_K\}\)</span>，则<strong>训练<span class="math inline">\(K\)</span>个分类器</strong> <span class="math inline">\(CLF_1,CLF_2,...,CLF_K\)</span>：</p>
<ul>
<li>训练<span class="math inline">\(CLF_i\)</span>时，将类别为 <span class="math inline">\(c_i\)</span> 的样本点定义为<strong>正类</strong>，将类别<strong>不是 <span class="math inline">\(c_i\)</span> </strong>的样本点定义为<strong>负类</strong>。</li>
<li>训练<span class="math inline">\(CLF_i\)</span>不光需要给出预测结果是否属于类别 <span class="math inline">\(c_i\)</span> ，还要给出<strong>置信度</strong>。</li>
</ul></li>
<li><p>预测时，对于未知的实例，用训练出来的<span class="math inline">\(K\)</span>个分类器来预测。</p>
<p>假设<strong>置信度最高</strong>的分类器为<span class="math inline">\(CLF_m\)</span>，则该实例的类别预测为 <span class="math inline">\(c_m\)</span> 。</p></li>
<li><p>缺点：非常容易<strong>陷入样本不平衡</strong>。</p>
<p>即使训练集中每一类样本都是平衡的，训练每个分类器时样本反而不平衡。</p></li>
</ol>
<h3 id="one-vs-one">5.2 one vs one</h3>
<ol type="1">
<li><p><strong>一对一</strong>：为<strong>每一对</strong>类别训练一个分类器。</p>
<p>假设类别为<span class="math inline">\(\{c_1,c_2,..,c_K\}\)</span>。那么<strong>训练 <span class="math inline">\(C_K^2=\frac{K(K-1)}{2}\)</span>个分类器 </strong></p>
<p><span class="math inline">\(CLF_{1,2},CLF_{1,3},...,CLF_{i,j},CLF_{K-1,K}\)</span>。</p>
<p>其中，<span class="math inline">\(CLF_{i,j},i&lt;j\)</span> 分类器从原始训练集中<strong>提取类别为<span class="math inline">\(c_i,c_j\)</span>的样本点</strong>作为<strong>新的训练集</strong>，然后训练<span class="math inline">\(CLF_{i,j}\)</span> 。</p></li>
<li><p>预测时，对于未知的实例，对预测结果进行<strong>投票</strong>。</p>
<ul>
<li>首先设投票结果为 <span class="math inline">\(s_0=0,s_1=0,..,s_K=0\)</span></li>
<li>然后用每个分类器<span class="math inline">\(CLF_{i,j}\)</span>对未知实例进行预测：
<ul>
<li>若预测结果是类别<span class="math inline">\(c_i\)</span>，则 <span class="math inline">\(s_i+=1\)</span>。</li>
<li>若预测结果是类别<span class="math inline">\(c_j\)</span>，则 <span class="math inline">\(s_j+=1\)</span>。</li>
<li>最终假设<span class="math inline">\(c_m\)</span>最大，则该未知的实例分类为<span class="math inline">\(c_m\)</span>。</li>
</ul></li>
</ul></li>
<li><p>缺点：需要训练的分类器数量为<span class="math inline">\(O(K^2)\)</span>，<strong>计算量太大</strong>。</p></li>
</ol>
<h3 id="many-vs-many">5.3 many vs many</h3>
<ol type="1">
<li><p>多对多：每次都将<strong>若干个类</strong>作为<strong>正类</strong>，<strong>若干个其他类</strong>作为<strong>反类</strong>。</p>
<ul>
<li>正、反类的构造必须有<strong>特殊的设计</strong>，不能随意选取。</li>
<li>通常采用纠错输出码<code>Error Correcting Output Codes:ECOC</code>技术。该技术将编码的思想引入类别拆分，并尽可能在解码过程中具有容错性。</li>
</ul></li>
<li><p><code>ECOC</code>工作过程主要分两步，假设类别为<span class="math inline">\(c_1,c_2,..,c_K\)</span>：</p>
<ul>
<li><p>编码：<strong>对<span class="math inline">\(K\)</span>个类别进行<span class="math inline">\(M\)</span>次划分</strong>，每次划分都将一部分类别划分为正类，一部分类别划分为反类，从而形成一个二分类训练集。</p>
<p>这样一个<strong>产生<span class="math inline">\(M\)</span>个训练集，可以训练出<span class="math inline">\(M\)</span>个分类器</strong>。</p></li>
<li><p>解码：用<span class="math inline">\(M\)</span>个分类器分别对测试样本进行预测，这些预测标记组成一个编码。</p>
<p>将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。</p></li>
</ul></li>
</ol>
<h2 id="六类别不平衡问题">六、类别不平衡问题</h2>
<ol type="1">
<li><p>通常在机器学习中都有一个基本假设：不同类别的训练样本数目相当。</p>
<ul>
<li><p>如果不同类别的训练样本数目稍有差别，通常影响不大。</p></li>
<li><p>如果不同类别的训练样本数目差别很大（极端情况下，如正类样本只有十个，反类样本一百万个），则会对学习过程造成影响。这就是类别不平衡问题(<code>class-imbalance</code>)。</p>
<p>这里讨论中，<strong>假设正类样本偏少、反类样本偏多</strong>。</p></li>
</ul></li>
<li><p>对于类别不平衡问题，常用的有三种方法：</p>
<ul>
<li>基于再缩放策略进行决策，称之为阈值移动<code>threshold-moving</code> 。</li>
<li>直接对训练集里的<strong>反类样本进行欠采样</strong><code>undersampling</code>。</li>
<li>直接对训练集里的<strong>正类样本进行过采样</strong><code>oversampling</code>。</li>
</ul></li>
<li><p>对于正负样本<strong>极不平衡</strong>的场景，可以完全换一个不同的角度来看问题：将它看作一分类<code>One Class Learning</code>或者异常检测<code>Novelty Detection</code>问题。</p>
<p>此时可以用<code>One-class SVM</code>模型。</p></li>
</ol>
<h3 id="欠采样">6.1 欠采样</h3>
<ol type="1">
<li><p>欠采样会<strong>去除一些反类</strong>使得正、反类数目接近。</p></li>
<li><p>欠采样若随机抛弃反类，则可能<strong>丢失一些重要信息</strong>。</p>
<p>常用方法是将反类<strong>划分成若干个集合</strong>供不同学习器使用，这样对每个学习器来看都是欠采样，但是全局来看并不会丢失重要信息。</p></li>
</ol>
<h3 id="过采样">6.2 过采样</h3>
<ol type="1">
<li><p>过采样会增加一些正类使得正、反类数目接近。</p></li>
<li><p>过采样<strong>不能简单</strong>的对原始正类进行重复采样，否则会导致严重的过拟合。</p>
<p>通常在原始正类之间<strong>插值</strong>来生成额外的正类。</p></li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title>降维</title>
    <url>/2020/03/08/%E9%99%8D%E7%BB%B4/</url>
    <content><![CDATA[<h1 id="降维">降维</h1>
<p>我们知道，解决过拟合的问题除了正则化和添加数据之外，降维就是最好的方法。降维的思路来源于维度灾难的问题，我们知道 <span class="math inline">\(n\)</span> 维球的体积为： <span class="math display">\[
CR^n
\]</span> 那么在球体积与边长为 <span class="math inline">\(2R\)</span> 的超立方体比值为： <span class="math display">\[
\lim\limits_{n\rightarrow0}\frac{CR^n}{2^nR^n}=0
\]</span></p>
<p>这就是所谓的维度灾难，在高维数据中，主要样本都分布在立方体的边缘，所以数据集更加稀疏。</p>
<p>降维的算法分为：</p>
<ol type="1">
<li>直接降维，特征选择</li>
<li>线性降维，PCA，MDS等</li>
<li>分线性，流形包括 Isomap，LLE 等</li>
</ol>
<p>为了方便，我们首先写出样本均值矩阵以及样本协方差矩阵： <span class="math display">\[
\bar X = \frac{1}{N}\sum\limits_{i = 1}^N { {x_i}}  = \frac{1}{N}\left( {\begin{array}{*{20}{c}}
  { {x_1}}&amp; \cdots &amp;{ {x_N}} 
\end{array}} \right)\left( {\begin{array}{*{20}{c}}
  1 \\ 
   \vdots  \\ 
  1 
\end{array}} \right) = \frac{1}{N}{X^T}{1_n}
\]</span></p>
<p><span class="math display">\[
\begin{align*}
S &amp;= \frac{1}{N}\sum\limits_{i = 1}^N {\left( { {x_i} - \bar x} \right)} \left( { {x_i} - \bar x} \right) \\
&amp;=\frac{1}{N}\left( {\begin{array}{*{20}{c}}
  { {x_1} - \bar x}&amp; \ldots &amp;{ {x_N} - \bar x} 
\end{array}} \right){\left( {\begin{array}{*{20}{c}}
  { {x_1} - \bar x}&amp; \ldots &amp;{ {x_N} - \bar x} 
\end{array}} \right)^T} \\
&amp;=\frac{1}{N}\left[ {\left( {\begin{array}{*{20}{c}}
  { {x_1}}&amp; \ldots &amp;{ {x_N}} 
\end{array}} \right) - \left( {\begin{array}{*{20}{c}}
  {\bar x}&amp; \ldots &amp;{\bar x} 
\end{array}} \right)} \right]{\left( {\begin{array}{*{20}{c}}
  { {x_1} - \bar x}&amp; \ldots &amp;{ {x_N} - \bar x} 
\end{array}} \right)^T} \\
&amp;= \frac{1}{N}\left( { {X^T} - \bar X1_n^T} \right){\left( {\begin{array}{*{20}{c}}
  { {x_1} - \bar x}&amp; \ldots &amp;{ {x_N} - \bar x} 
\end{array}} \right)^T}\\
&amp;= \frac{1}{N}\left( { {X^T} - \frac{1}{N}{X^T}{1_n}1_n^T} \right){\left( {\begin{array}{*{20}{c}}
  { {x_1} - \bar x}&amp; \ldots &amp;{ {x_N} - \bar x} 
\end{array}} \right)^T}\\
&amp;= \frac{1}{N}{X^T}\left( { {I_n} - \frac{1}{N}{1_n}1_n^T} \right){\left( { {I_n} - \frac{1}{N}{1_n}1_n^T} \right)^T}X\\
&amp;=\frac{1}{N}{X^T}H{H^T}X \\
\end{align*}
\]</span></p>
<p>其中，<span class="math inline">\(H = \left( { {I_n} - \frac{1}{N}{1_n}1_n^T} \right)\)</span> ，且有性质<span class="math inline">\({H^T} = H = {H^2} = {H^n}\)</span> 。</p>
<p>所以： <span class="math display">\[
\begin{align*}
S&amp;=\frac{1}{N}X^THH^TX\nonumber\\
&amp;=\frac{1}{N}X^THHX=\frac{1}{N}X^THX
\end{align*}
\]</span> 这个式子利用了中心矩阵 $ H$的对称性，这也是一个投影矩阵。</p>
<h2 id="主成分分析-pca">主成分分析 PCA</h2>
<h3 id="损失函数">损失函数</h3>
<p>主成分分析中，我们的基本想法是将所有数据投影到一个字空间中，从而达到降维的目标，为了寻找这个子空间，我们基本想法是：</p>
<ol type="1">
<li>所有数据在子空间中更为分散（最大投影方差）</li>
<li>损失的信息最小，即：在补空间的分量少（最小重构距离）</li>
</ol>
<p>原来的数据很有可能各个维度之间是相关的，于是我们希望找到一组 <span class="math inline">\(p\)</span> 个新的线性无关的单位正交基 <span class="math inline">\(u_i\)</span>，降维就是取其中的 <span class="math inline">\(q\)</span> 个基。于是对于一个样本 <span class="math inline">\(x_i\)</span>，经过这个坐标变换后： <span class="math display">\[
\hat{x_i}=\sum\limits_{i=1}^p({x_{_i}^T{u_i}})u_i=\sum\limits_{i=1}^q({x_{_i}^T{u_i}})u_i+\sum\limits_{i=q+1}^p({x_{_i}^T{u_i}})u_i
\]</span></p>
<h3 id="最大投影方差">最大投影方差</h3>
<p>对于数据集来说，我们首先将其中心化然后再带入上面的式子的第一项，并使用其系数的平方平均作为损失函数并最大化： <span class="math display">\[
\begin{align}
J&amp;=\frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{j=1}^q((x_i-\overline{x})^Tu_j)^2\nonumber\\
&amp;= \frac{1}{N}{\sum\limits_{i = 1}^N {\sum\limits_{j = 1}^q {\left( { { {\left( { {x_i} - \bar x} \right)}^T}{u_j}} \right)} } ^T}{\left( { {x_i} - \bar x} \right)^T}{u_j}\\
&amp;= \frac{1}{N}\sum\limits_{i = 1}^N {\sum\limits_{j = 1}^q { {u_j}^T\left( { {x_i} - \bar x} \right)} } {\left( { {x_i} - \bar x} \right)^T}{u_j}\\
&amp;=\sum\limits_{j=1}^qu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align}
\]</span> 由于每个基都是线性无关的，于是每一个 <span class="math inline">\(u_j\)</span> 的求解可以分别进行，使用拉格朗日乘子法： <span class="math display">\[
\mathop{argmax}_{u_j}L(u_j,\lambda)=\mathop{argmax}_{u_j}u_j^TSu_j-\lambda(u_j^Tu_j-1)
\]</span> 于是将上式拉格朗日函数对<span class="math inline">\(u_j\)</span> 求导可得： <span class="math display">\[
Su_j=\lambda u_j
\]</span> 可见，我们需要的基就是协方差矩阵的特征向量。损失函数最大取在特征值前 <span class="math inline">\(q\)</span> 个最大值，所对应的特征向量即为主成分。</p>
<h3 id="最小重构距离">最小重构距离</h3>
<p>下面看其损失的信息最少（最小重构距离）这个条件，假设数据一共 <span class="math inline">\(p\)</span> 维，需要降维到 <span class="math inline">\(q\)</span> ，同样适用系数的平方平均作为损失函数（注意求和范围是从$q+1 $ 开始），并最小化： <span class="math display">\[
{x_i} = \sum\limits_{j = 1}^q {\left( {x_i^T{u_j}} \right)\overrightarrow { {u_j}} }  = \sum\limits_{j = 1}^p {\left( {x_i^T{u_j}} \right)} {u_j} \\
{\hat x_i} = \sum\limits_{j = 1}^q {\left( {x_i^T{u_j}} \right)} {u_j}
\]</span></p>
<p><span class="math display">\[
\begin{align*}
J&amp;=\frac{1}{N}{\sum\limits_{i = 1}^N {\left\| { {x_i} - { {\hat x}_i}} \right\|} ^2} = \frac{1}{N}{\sum\limits_{i = 1}^N {\left\| {\sum\limits_{j = q + 1}^p {\left( {x_i^T{u_j}} \right){u_j}} } \right\|} ^2} \\
&amp;=\frac{1}{N}\sum\limits_{i=1}^N\sum\limits_{j=q+1}^p((x_i-\overline{x})^Tu_j)^2\nonumber\\
&amp;=\sum\limits_{j=q+1}^pu_j^TSu_j\ ,\ s.t.\ u_j^Tu_j=1
\end{align*}
\]</span></p>
<p>同样的： <span class="math display">\[
\mathop{argmin}_{u_j}L(u_j,\lambda)=\mathop{argmin}_{u_j}u_j^TSu_j+\lambda(1-u_j^Tu_j)
\]</span> 损失函数最小取在本征值剩下的最小的几个特征值。数据集的协方差矩阵可以写成 <span class="math inline">\(S=U\Lambda U^T\)</span> （对称矩阵的奇异值分解），直接对这个表达式当然可以得到特征向量和特征值。</p>
<h3 id="svd-与-pcoa">SVD 与 PCoA</h3>
<p>下面使用实际训练时常使用的 SVD（奇异值分解） 直接求得这个 <span class="math inline">\(q\)</span> 个特征向量。</p>
<p>对中心化后的数据集即<span class="math inline">\(HX\)</span>进行奇异值分解： <span class="math display">\[
HX=U\Sigma V^T,U^TU=I_N,V^TV=I_p,\Sigma:N\times p
\]</span></p>
<p>于是： <span class="math display">\[
\begin{align*}
S&amp;=\frac{1}{N}X^THX \\
&amp;=\frac{1}{N}X^TH^THX \\
&amp;=\frac{1}{N}{\left( {HX} \right)^T}HX \\
&amp;= \frac{1}{N}V\Sigma^T {U^T}U\Sigma {V^T}\\
&amp;=\frac{1}{N}V\Sigma^T\Sigma V^T \\
&amp;= \frac{1}{N}V{\Sigma ^2}{V^T}
\end{align*}
\]</span> 因此，我们直接对中心化后的数据集进行 SVD，就可以得到特征值 <span class="math inline">\(\Sigma\)</span> 和特征向量 <span class="math inline">\(V\)</span>，在新坐标系中的坐标就是： <span class="math display">\[
HX\cdot V
\]</span> 由上面的推导，我们也可以得到另一种方法 PCoA(Principle Coordinate Analysis) 主坐标分析，定义并进行特征值分解： <span class="math display">\[
\begin{align*}
T&amp;=HXX^TH \\
&amp;= U\Sigma {V^T}V\Sigma {U^T}\\
&amp;=U\Sigma\Sigma^TU^T \\
&amp;=U{\Sigma ^2}{U^T}
\end{align*}
\]</span> 由于： <span class="math display">\[
\begin{align*}
T(U\Sigma)&amp;=U{\Sigma ^2}{U^T}U\Sigma  \\
&amp;=U\Sigma(\Sigma^2) \\
\end{align*}
\]</span> 于是可以直接得到坐标。这两种方法都可以得到主成分，但是由于方差矩阵是 <span class="math inline">\(p\times p\)</span> 的，而 <span class="math inline">\(T\)</span> 是 <span class="math inline">\(N\times N\)</span> 的，所以对样本量较少，维度<span class="math inline">\(p\)</span> 较大的时候可以采用 PCoA的方法。</p>
<h3 id="p-pca">p-PCA</h3>
<p>下面从概率的角度对 PCA 进行分析，概率方法也叫 p-PCA。我们使用线性模型（<span class="math inline">\(x\)</span> 与 <span class="math inline">\(z\)</span> 是线性关系），我们选定一个方向，对原数据 <span class="math inline">\(x\in\mathbb{R}^p\)</span> ，降维后的数据为 <span class="math inline">\(z\in\mathbb{R}^q,q&lt;p\)</span>。假设 <span class="math inline">\(z\)</span> 有一个先验，且 <span class="math inline">\(z\)</span> 与$ $ 相互独立： <span class="math display">\[
\begin{align*}
z&amp;\sim\mathcal{N}(\mathbb{O}_{q1},\mathbb{I}_{q*q})\\
x&amp;=Wz+\mu+\varepsilon\\
\varepsilon&amp;\sim\mathcal{N}(0,\sigma^2\mathbb{I}_{p*p}), \varepsilon \bot z\\
\end{align*}
\]</span> 对于这个模型，我么可以使用期望-最大（EM）的算法进行学习，在进行推断的时候需要求得 <span class="math inline">\(p(z|x)\)</span>，推断的求解过程和线性高斯模型类似。</p>
<p>复习一下线性高斯模型中得到的结论：</p>
<ol type="1">
<li><p><span class="math inline">\(x_a=\begin{pmatrix}\mathbb{I}_{m\times m}&amp;\mathbb{O}_{m\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}\)</span>，代入定理中得到： <span class="math display">\[
\mathbb{E}[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_a\\
Var[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\\mathbb{O}\end{pmatrix}=\Sigma_{aa}
\]</span> 所以 <span class="math inline">\(x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})\)</span>。</p></li>
<li><p>同样的，<span class="math inline">\(x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})\)</span>。</p></li>
<li><p>对于两个条件概率，我们引入三个量： <span class="math display">\[
x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
\mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\\
\Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}
\]</span> 特别的，最后一个式子叫做 <span class="math inline">\(\Sigma_{bb}\)</span> 的 Schur Complementary。可以看到： <span class="math display">\[
x_{b\cdot a}=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\\x_b\end{pmatrix}
\]</span> 所以： <span class="math display">\[
\mathbb{E}[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\\mu_b\end{pmatrix}=\mu_{b\cdot a}\\
Var[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\\mathbb{I}_{n\times n}\end{pmatrix}=\Sigma_{bb\cdot a}
\]</span> 利用这三个量可以得到 <span class="math inline">\(x_b=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a\)</span>。因此： <span class="math display">\[
\begin{align*}
\mathbb{E}[x_b|x_a]&amp;=\mathbb{E}[x_{b\cdot a}] + \Sigma_{ba}\Sigma_{aa}^{-1}x_a\\
&amp;=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a  \\
&amp;=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a+\Sigma_{ba}\Sigma_{aa}^{-1}x_a \\
&amp;= {\mu _b}{\text{ + }}{\Sigma _{ba}}\Sigma _{aa}^{ - 1}\left( { {x_a}{-}{\mu _a}} \right)
\end{align*}
\]</span></p>
<p><span class="math display">\[
Var[x_b|x_a]=Var[x_{b \cdot a}]=\Sigma_{bb\cdot a}
\]</span></p>
<p>这里同样用到了定理。</p></li>
<li><p>同样： <span class="math display">\[
x_{a\cdot b}=x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\\
\mu_{a\cdot b}=\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\\
\Sigma_{aa\cdot b}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}
\]</span> 所以： <span class="math display">\[
\begin{align*}
\mathbb{E}[x_a|x_b]&amp;=\mathbb{E}[x_{a\cdot b}] + \Sigma_{ab}\Sigma_{bb}^{-1}x_b \\
&amp;=\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b \\
&amp;=\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b+\Sigma_{ab}\Sigma_{bb}^{-1}x_b\\
&amp;={\mu _a}{\text{ + }}{\Sigma _{ab}}\Sigma _{bb}^{ - 1}\left( { {x_b}{-}{\mu _b}} \right)
\end{align*}
\]</span></p>
<p><span class="math display">\[
Var[x_a|x_b]=\Sigma_{aa\cdot b}
\]</span></p></li>
</ol>
<p>下面利用上边四个量，求解线性模型： <span class="math display">\[
\begin{align}
&amp;p(z|x)=\frac{p(x|z)p(z)}{p(x)}\\
&amp;\mathbb{E}[x|z]=\mathbb{E}[wz+\mu + \varepsilon]=wz+\mu\\
&amp;Var[x|z]=Var[wz+\mu + \varepsilon]=Var[\varepsilon]=\sigma^2\mathbb{I}_{p*p}\\
&amp;\mathbb{E}[x]=\mathbb{E}[Wz+\mu+\varepsilon]=\mathbb{E}[wz+\mu]+\mathbb{E}[\varepsilon]=w*0+\mu+0=\mu\\
&amp;Var[x]=Var[Wz+\mu+\varepsilon]=Var[Wz+\mu]+Var[\varepsilon]=W\mathbb{I}_{q*q}W^T+\sigma^2\mathbb{I}_{p*p}=WW^T+\sigma^2\mathbb{I}_{p*p}\\
\end{align}
\]</span></p>
<p>所以<span class="math inline">\(x \sim \mathcal{N}\left( {\mu ,W{W^T} + {\sigma ^2}{\mathbb{I}_{p*p}}} \right)\)</span> 。</p>
<p>接着，为了套线性高斯模型公式，我们构造： <span class="math display">\[
\left( {\begin{array}{*{20}{c}}
  x \\ 
  z 
\end{array}} \right) \sim \mathcal{N}\left( {\left[ {\begin{array}{*{20}{c}}
  \mu  \\ 
  0 
\end{array}} \right],\left[ {\begin{array}{*{20}{c}}
  {W{W^T} + {\sigma ^2}\mathbb{I}_{p*p}}&amp;{Cov(x,z)} \\ 
  {Cov(z,x)}&amp;\mathbb{I}_{q*q} 
\end{array}} \right]} \right)
\]</span></p>
<p><span class="math display">\[
\begin{align*}
Cov(x,z) &amp;= Cov(z,x) = \mathbb{E}\left[ {\left( {x - \mu } \right){ {\left( {z - 0} \right)}^T}} \right]\\
&amp;=\mathbb{E}\left[ {\left( {x - \mu } \right){z^T}} \right] = \mathbb{E}\left[ {\left( {Wz + \mu  + \varepsilon  - \mu } \right){z^T}} \right] \\
&amp;= \mathbb{E}\left[ {\left( {Wz + \varepsilon } \right){z^T}} \right] = \mathbb{E}\left[ {Wz{z^T} + \varepsilon {z^T}} \right]\\
&amp;=\mathbb{E}\left[ {Wz{z^T}} \right] + \mathbb{E}\left[ \varepsilon  \right] \times \mathbb{E}\left[ { {z^T}} \right]\\
&amp;=W\mathbb{E}\left[ {\left( {z - 0} \right){ {(z - 0)}^T}} \right] + 0 \times \mathbb{E}\left[ { {z^T}} \right]\\
&amp;=WVar\left[ z \right] = W{\mathbb{I}_{q*q}} = W
\end{align*}
\]</span></p>
<p>所以，代入线性高斯模型： <span class="math display">\[
\begin{align*}
&amp;{\mu _a} = \mu ,{\mu _b} = 0\\
&amp;{\Sigma _{ab}} = {\Sigma _{ba}} = W,{\Sigma _{aa}} = W{W^T} + {\sigma ^2}{\mathbb{I}_{p*p}},{\Sigma _{bb}} = {\mathbb{I}_{q*q}}\\
&amp;{\Sigma _{bb \cdot a}} = {\Sigma _{bb}} - {\Sigma _{ba}}\Sigma _{aa}^{ - 1}{\Sigma _{ab}} = {\mathbb{I}_{q*q}} - W{\left( {W{W^T} + {\sigma ^2}} \right)^{ - 1}}W
\end{align*}
\]</span> 最终： <span class="math display">\[
\begin{align*}
\because 
&amp;\mathbb{E}[x_b|x_a]={\mu _b}{\text{ + }}{\Sigma _{ba}}\Sigma _{aa}^{ - 1}\left( { {x_a}{-}{\mu _a}} \right) \\
&amp; Var[x_b|x_a]=\Sigma_{bb\cdot a}\\
\therefore &amp;z|x;W,\mu ,{\sigma ^2} \sim \mathcal{N}(W{(W{W^T} + {\sigma ^2}{\mathbb{I}_{p*p}})^{ - 1}}(x - \mu ),{\mathbb{I}_{q*q}} - W{(W{W^T} + {\sigma ^2}{\mathbb{I}_{p*p}})^{ - 1}}W)
\end{align*}
\]</span> 现在已经根据联合分布求出条件分布，只要根据似然函数求出参数即可。</p>
<h2 id="小结">小结</h2>
<p>降维是解决维度灾难和过拟合的重要方法，除了直接的特征选择外，我们还可以采用算法的途径对特征进行筛选，线性的降维方法以 PCA 为代表，在 PCA 中，我们只要直接对数据矩阵进行中心化然后求奇异值分解或者对数据的协方差矩阵进行分解就可以得到其主要维度。非线性学习的方法如流形学习将投影面从平面改为超曲面。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title>线性分类</title>
    <url>/2020/03/06/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB/</url>
    <content><![CDATA[<h1 id="线性分类">线性分类</h1>
<p>对于分类任务，线性回归模型就无能为力了，但是我们可以在线性模型的函数进行后再加入一层激活函数，这个函数是非线性的，激活函数的反函数叫做链接(Link)函数。我们有两种线性分类的方式：</p>
<ol type="1">
<li>硬分类，我们直接需要输出观测对应的分类。这类模型的代表为：
<ul>
<li><p>线性判别分析（Fisher 判别）</p></li>
<li><p>感知机</p></li>
</ul></li>
<li>软分类，产生不同类别的概率，这类算法根据获得后验概率方法的不同分为两种
<ul>
<li><p>生成式（先对联合概率分布<span class="math inline">\(P\left( {x,c} \right)\)</span> 建模，然后再由此获得<span class="math inline">\(P\left( {c|x} \right)\)</span>）：高斯判别分析（GDA）和朴素贝叶斯等为代表</p>
<ul>
<li><p>GDA</p></li>
<li><p>Naive Bayes</p></li>
</ul></li>
<li><p>判别式（直接对条件概率<span class="math inline">\(P\left( {c|x} \right)\)</span> 进行建模来预测<span class="math inline">\(c\)</span> ）：Logistic 回归</p></li>
</ul></li>
</ol>
<h2 id="硬分类-感知机">硬分类-感知机</h2>
<p>假设输入空间<span class="math inline">\(\mathcal{X} \subseteq \mathbb{R}^{p}\)</span>，输出空间<span class="math inline">\(\mathcal{Y} = \left\{+1, -1 \right\}\)</span>。输入<span class="math inline">\(x \in \mathcal{X}\)</span>表示实例的特征向量，对应于输入空间的点；输出<span class="math inline">\(y \in \mathcal{Y}\)</span>表示实例的类别。由输入空间到输出空间的函数 <span class="math display">\[
f \left( x \right) = sign \left( w \cdot x \right)
\]</span> 称为感知机。其中，<span class="math inline">\(w\)</span>和<span class="math inline">\(b\)</span>为感知机模型参数，<span class="math inline">\(w \in \mathbb{R}^{p}\)</span>叫做权值或权值向量，<span class="math inline">\(w \cdot x\)</span>表示<span class="math inline">\(w\)</span>和<span class="math inline">\(x\)</span>的内积。<span class="math inline">\(sign\)</span>是符号函数，即 <span class="math display">\[
sign \left( x \right) = \left\{
\begin{aligned} 
\ &amp;  +1, x \geq 0
\\ &amp; -1, x&lt;0
\end{aligned}
\right.
\]</span></p>
<p>感知机是一种线性分类模型，属于<strong>判别模型</strong>。感知机模型的假设空间是定义在特征空间中的所有线性分类模型或线性分类器，即函数集合<span class="math inline">\(\left\{ f | f \left( x \right) = w \cdot x \right\}\)</span>。</p>
<p>感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数<span class="math inline">\(w\)</span>，需要确定一个学习策略，即定义损失函数并将损失函数极小化。</p>
<p>损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数<span class="math inline">\(w\)</span> 的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面<span class="math inline">\(S\)</span> 的总距离，这是感知机所采用的。输入空间<span class="math inline">\(R^{n}\)</span>中的任一点<span class="math inline">\(x_{0}\)</span>到超平面<span class="math inline">\(S\)</span>的距离：</p>
<p><span class="math display">\[
\dfrac{1}{\| w \|} \left| {w^T}  x_{0} \right|
\]</span> 其中<span class="math inline">\(\| w \|\)</span>是<span class="math inline">\(w\)</span>的<span class="math inline">\(L_{2}\)</span>范数。</p>
<p>对于误分类数据<span class="math inline">\(\left( x_{i}, y_{i} \right)\)</span>，当<span class="math inline">\({w^T}x &gt; 0\)</span>时，<span class="math inline">\(y_{i}=-1\)</span>，当<span class="math inline">\({w^T} x &lt; 0\)</span>时，<span class="math inline">\(y_{i}=+1\)</span>，有 <span class="math display">\[
-y_{i} \left( {w^T}x_{i} \right) &gt; 0
\]</span> 误分类点<span class="math inline">\(x_{i}\)</span>到分离超平面的距离: <span class="math display">\[
-\dfrac{1}{\| w \|} y_{i}\left({w^T} x_{i} \right)
\]</span></p>
<p>假设超平面<span class="math inline">\(S\)</span>的误分类点集合为<span class="math inline">\(M\)</span>，则所有误分类点到超平面<span class="math inline">\(S\)</span> 的总距离： <span class="math display">\[
-\dfrac{1}{\| w \|} \sum_{x_{i} \in M} y_{i} \left({w^T}x_{i}\right)
\]</span> 不考虑<span class="math inline">\(\dfrac{1}{\| w \|}\)</span> ，就得到感知机学习的损失函数： <span class="math display">\[
L(w)=\sum\limits_{ {x_{i} \in M}}-y_iw^Tx_i
\]</span> 其中<span class="math inline">\(M\)</span> 为误分类点集合。</p>
<p>显然，损失函数<span class="math inline">\(L(w)\)</span> 是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。实际在每一次训练的时候，我们采用梯度下降的算法。损失函数对 <span class="math inline">\(w\)</span> 的偏导为： <span class="math display">\[
\frac{\partial}{\partial w}L(w)=\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_ix_i
\]</span> 但是如果样本非常多的情况下，计算复杂度较高，但是，实际上我们并不需要绝对的损失函数下降的方向，我们只需要损失函数的期望值下降，但是计算期望需要知道真实的概率分布，我们实际只能根据训练数据抽样来估算这个概率分布即经验风险（关于训练集的平均损失）： <span class="math display">\[
\mathbb{E}_{\mathcal D}[\mathbb{E}_\hat{p}[\nabla_wL(w)]]=\mathbb{E}_{\mathcal D}[\frac{1}{N}\sum\limits_{i=1}^N\nabla_wL(w)]
\]</span> 我们知道， <span class="math inline">\(N\)</span> 越大，样本近似真实分布越准确，但是对于一个标准差为 <span class="math inline">\(\sigma\)</span> 的数据，可以确定的标准差仅和 <span class="math inline">\(\sqrt{N}\)</span> 成反比，而计算速度却和 <span class="math inline">\(N\)</span> 成正比。因此可以每次使用较少样本，则在数学期望的意义上损失降低的同时，有可以提高计算速度，如果每次只使用一个错误样本，我们有下面的更新策略（根据泰勒公式，在负方向）： <span class="math display">\[
w^{t+1}\leftarrow w^{t}+\lambda y_ix_i
\]</span> 是可以收敛的，同时使用单个观测更新也可以在一定程度上增加不确定度，从而减轻陷入局部最小的可能。在更大规模的数据上，常用的是小批量随机梯度下降法。</p>
<h2 id="硬分类-线性判别分析">硬分类-线性判别分析</h2>
<p>在线性判别分析(LDA) 中，我们的基本想法是选定一个方向，将试验样本顺着这个方向投影，投影后的数据需要满足两个条件，从而可以更好地分类：</p>
<ol type="1">
<li>相同类内部的试验样本距离接近。</li>
<li>不同类别之间的距离较大。</li>
</ol>
<p>首先是投影，我们假定原来的数据是向量 <span class="math inline">\(x\)</span>，那么顺着 $ w$ 方向的投影就是标量： <span class="math display">\[
z=w^T\cdot x(=|w|\cdot|x|\cos\theta)
\]</span> 对第一点，相同类内部的样本更为接近，我们假设属于两类的试验样本数量分别是 <span class="math inline">\(N_1\)</span>和 <span class="math inline">\(N_2\)</span>，那么我们采用方差矩阵来表征每一个类内的总体分布，这里我们使用了方差的定义，用 <span class="math inline">\(S\)</span> 表示原数据的方差： <span class="math display">\[
\begin{align}
C_1:Var_z[C_1]&amp;=\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(z_i-\overline{z_{c1}})(z_i-\overline{z_{c1}})^T\nonumber\\
&amp;=\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(w^Tx_i-\frac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)(w^Tx_i-\frac{1}{N_1}\sum\limits_{j=1}^{N_1}w^Tx_j)^T\nonumber\\
&amp;=\frac{1}{N_{1}} \sum_{i=1}^{N_{1}} w^{T}\left(x_{i}-\frac{1}{N_{1}} \sum_{j=1}^{N_{1}} x_{j}\right)\left(x_{i}-\frac{1}{N_{1}} \sum_{j=1}^{N_{1}} x_{j}\right)^{T}({w^T})^{T}\nonumber\\
&amp;=w^T\frac{1}{N_1}\sum\limits_{i=1}^{N_1}(x_i-\overline{x_{c1}})(x_i-\overline{x_{c1}})^Tw\nonumber\\
&amp;=w^TS_{c1}w\\
C_2:Var_z[C_2]&amp;=\frac{1}{N_2}\sum\limits_{i=1}^{N_2}(z_i-\overline{z_{c2}})(z_i-\overline{z_{c2}})^T\nonumber\\
&amp;=w^TS_{c2}w
\end{align}
\]</span> 所以类内距离可以记为： <span class="math display">\[
\begin{align}
Var_z[C_1]+Var_z[C_2]=w^T(S_1+S_2)w
\end{align}
\]</span> 对于第二点，我们可以用两类的均值表示这个距离： <span class="math display">\[
\begin{align}
(\overline{z_{c1}}-\overline{z_{c2}})^2&amp;=(\frac{1}{N_1}\sum\limits_{i=1}^{N_1}w^Tx_i-\frac{1}{N_2}\sum\limits_{i=1}^{N_2}w^Tx_i)^2\nonumber\\
&amp;=(w^T(\overline{x_{c1}}-\overline{x_{c2}}))^2\nonumber\\
&amp;=w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw
\end{align}
\]</span> 综合这两点，由于协方差是一个矩阵，于是我们用将这两个值相除来得到我们的损失函数，并最大化这个值： <span class="math display">\[
\begin{align}
\hat{w}=\mathop{argmax}\limits_wJ(w)&amp;=\mathop{argmax}\limits_w\frac{(\overline{z_{c1}}-\overline{z_{c2}})^2}{Var_z[C_1]+Var_z[C_2]}\nonumber\\
&amp;=\mathop{argmax}\limits_w\frac{w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw}{w^T(S_{c1}+S_{c2})w}\nonumber\\
&amp;=\mathop{argmax}\limits_w\frac{w^TS_bw}{w^TS_ww}
\end{align}
\]</span> 这样，我们就把损失函数和原数据集以及参数结合起来了。下面对这个损失函数求偏导： <span class="math display">\[
\begin{align}
\frac{\partial}{\partial w}J(w)&amp;=\frac{\partial}{\partial w}{w^T}{S_b}w{({w^T}{S_w}w)^{ - 1}} \nonumber\\
&amp;=2S_bw(w^TS_ww)^{-1}-2w^TS_bw(w^TS_ww)^{-2}S_ww=0\nonumber\\
&amp;\Longrightarrow S_bw(w^TS_ww)=(w^TS_bw)S_ww\nonumber\\
\end{align}
\]</span> 因为<span class="math inline">\(w:p*1\)</span>, <span class="math inline">\(S_w:p*p\)</span>, <span class="math inline">\(S_b:p*p\)</span> ，所以<span class="math inline">\({w^T}{S_b}w\)</span> 与<span class="math inline">\({w^T}{S_w}w\)</span> 是<span class="math inline">\(1*1\)</span>的scalar实数。并注意我们其实对 <span class="math inline">\(w\)</span> 的绝对值没有任何要求，只对方向有要求，所以： <span class="math display">\[
\begin{align*}
&amp;\Longrightarrow S_ww=\frac{w^TS_ww}{w^TS_bw}S_bw \\
&amp;\Longrightarrow w=\frac{w^TS_ww}{w^TS_bw}S_w^{-1}S_bw \propto S_w^{-1}S_bw=S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw\\
\end{align*}
\]</span> 又因为根据矩阵乘法结合律：<span class="math inline">\((\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw = (\overline{x_{c1}}-\overline{x_{c2}})((\overline{x_{c1}}-\overline{x_{c2}})^Tw)\)</span> ，而<span class="math inline">\((\overline{x_{c1}}-\overline{x_{c2}})^T\)</span> 的维度是<span class="math inline">\(1*p\)</span>，<span class="math inline">\(w\)</span> 的维度是<span class="math inline">\(p*1\)</span>，即<span class="math inline">\((\overline{x_{c1}}-\overline{x_{c2}})^Tw\)</span> 是一个scalar实数。所以： <span class="math display">\[
w\propto S_w^{-1}(\overline{x_{c1}}-\overline{x_{c2}})
\]</span> 于是 $ S_w^{-1}(-)$ 就是我们需要寻找的方向。最后可以归一化求得单位的 <span class="math inline">\(w\)</span> 值。</p>
<h2 id="软分类-logistic-回归">软分类-Logistic 回归</h2>
<p>Logistic回归是概率判别模型。有时候我们只要得到一个类别的概率，那么我们需要一种能输出 <span class="math inline">\([0,1]\)</span> 区间的值的函数。考虑两分类模型，我们利用判别模型，希望对 <span class="math inline">\(p(C|x)\)</span> 建模，利用贝叶斯定理： <span class="math display">\[
p(C_1|x)=\frac{p(x|C_1)p(C_1)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)}
\]</span> 取 <span class="math inline">\(a=\ln\frac{p(x|C_1)p(C_1)}{p(x|C_2)p(C_2)}\)</span>，于是： <span class="math display">\[
p(C_1|x)=\frac{1}{1+\exp(-a)}
\]</span> 上面的式子叫Sigmoid 函数，其参数表示了两类联合概率比值的对数。在判别式中，不关心这个参数的具体值，模型假设直接对 <span class="math inline">\(a\)</span> 进行。</p>
<p>Logistic 回归的模型假设是： <span class="math display">\[
a=w^Tx
\]</span> 于是，通过寻找 $  w$ 的最佳值可以得到在这个模型假设下的最佳模型。概率判别模型常用最大似然估计的方式来确定参数。</p>
<p>对于一次观测，获得分类 <span class="math inline">\(y\)</span> 的概率为（假定<span class="math inline">\(C_1=1,C_2=0\)</span>）： <span class="math display">\[
p(y|x)=p_1^yp_0^{1-y}
\]</span></p>
<p>其中 ，</p>
<p><span class="math display">\[
{p_1} = P\left( {y = 1|x} \right) = Sigmoid\left( { {w^T}x} \right) = \frac{1}{ {1 + {e^{ - {w^T}x}}}}  \\
{p_0} = P\left( {y = 0|x} \right) = 1 - P(y = 1|x) = 1 - \frac{1}{ {1 + {e^{ - {w^T}x}}}}
\]</span></p>
<p>那么对于 <span class="math inline">\(N\)</span> 次独立同分布的观测 log-MLE为： <span class="math display">\[
\begin{align*}
\hat{w}=\mathop{argmax}_wJ(w)&amp;=\mathop{argmax}_w \log P\left( {Y|X} \right)   \\
&amp;=\mathop{argmax}_w \log \prod\limits_{i = 1}^N {P\left( { {y_i}|{x_i}} \right)} \\
&amp;=\mathop{argmax}_w \sum\limits_{i = 1}^N {\log P\left( { {y_i}|{x_i}} \right)} \\
&amp;=\mathop{argmax}_w \sum\limits_{i = 1}^N { {y_i}\log {p_1} + \left( {1 - {y_i}} \right)\log {p_0}}  \\
\end{align*}
\]</span> 注意到，这个表达式是交叉熵表达式的相反数乘 <span class="math inline">\(N\)</span>，MLE 中的对数(又叫对数似然)也保证了可以和指数函数相匹配，从而在大的区间汇总获取稳定的梯度。</p>
<p>对这个函数求导数，注意到： <span class="math display">\[
g&#39;=(\frac{1}{1+\exp(-c)})&#39;=g(1-g)
\]</span> 则： <span class="math display">\[
\begin{align*}
J&#39;(w)&amp;=\sum\limits_{i = 1}^N { {y_i}} p_1^{ - 1}.{p_1}\left( {1 - {p_1}} \right){x_i} + \left( {1 - {y_i}} \right)\left( { - {p_1}} \right){x_i} \\
&amp;=\sum\limits_{i=1}^Ny_i(1-p_1)x_i-p_1x_i+y_ip_1x_i \\
&amp;=\sum\limits_{i=1}^N(y_i-p_1)x_i
\end{align*}
\]</span> 由于概率值的非线性，放在求和符号中时，这个式子无法直接求解。于是在实际训练的时候，和感知机类似，也可以使用不同大小的批量随机梯度上升（对于最小化就是梯度下降）来获得这个函数的极大值。</p>
<h2 id="软分类-高斯判别分析">软分类-高斯判别分析</h2>
<p>高斯判别分析(GDA)是概率生成模型。在生成模型中，我们对联合概率分布进行建模，然后采用 MAP 来获得参数的最佳值。在GDA中，针对两分类的情况，我们采用的假设：</p>
<ol type="1">
<li><span class="math inline">\(y\sim Bernoulli(\phi)\)</span></li>
<li><span class="math inline">\(x|y=1\sim\mathcal{N}(\mu_1,\Sigma)\)</span></li>
<li><span class="math inline">\(x|y=0\sim\mathcal{N}(\mu_0,\Sigma)\)</span></li>
</ol>
<p>那么独立同分布的数据集最大后验概率估计MAP可以表示为： <span class="math display">\[
\hat y = \mathop{argmax}_{y \in \left\{ {0,1} \right\}}P\left( {y|x} \right) =\mathop{argmax}_y P\left( y \right)P\left( {x|y} \right)
\]</span> 于是： <span class="math display">\[
\begin{align}
\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\log p(X|Y)p(Y)&amp;=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N (\log p(x_i|y_i)+\log p(y_i))\nonumber\\
&amp;=\mathop{argmax}_{\phi,\mu_0,\mu_1,\Sigma}\sum\limits_{i=1}^N(\log\mathcal{N}(\mu_0,\Sigma)^{1-y_i}+\log \mathcal{N}(\mu_1,\Sigma)^{y_i}+\log\phi^{y_i}(1-\phi)^{1-y_i})
\end{align}
\]</span></p>
<ul>
<li><p>首先对 <span class="math inline">\(\phi\)</span> 进行求解， <span class="math display">\[
\begin{align}
\sum\limits_{i = 1}^N {\log {\phi ^{ {y_i}}}{ {(1 - \phi )}^{1 - {y_i}}}}&amp;=\sum\limits_{i = 1}^N { {y_i}\log \phi  + \left( {1 - {y_i}} \right)\log (1 - \phi )}  \\
\end{align}
\]</span> 将上式对 <span class="math inline">\(\phi\)</span> 求偏导： <span class="math display">\[
\sum\limits_{i = 1}^N {\frac{ { {y_i}}}{\phi }}  + \left( {1 - {y_i}} \right)\frac{1}{ {1 - \phi }}\left( { - 1} \right) = 0 \\
\Longrightarrow  \sum\limits_{i = 1}^N { {y_i}\left( {1 - \phi } \right) - \left( {1 - {y_i}} \right)\phi }  = 0\\
\Longrightarrow\phi=\frac{\sum\limits_{i=1}^Ny_i}{N}=\frac{N_1}{N}
\]</span></p></li>
<li><p>然后求解 <span class="math inline">\(\mu_1\)</span>： <span class="math display">\[
\begin{align*}
\hat{\mu_1}&amp;=\mathop {argmax}\limits_{ {\mu _1}} \sum\limits_{i = 1}^N {\log \mathcal{N}{ {({\mu _1},\Sigma )}^{ {y_i}}}}  \\
&amp;=\mathop{argmax}_{\mu_1}\sum\limits_{i=1}^Ny_i\log\mathcal{N}(\mu_1,\Sigma)\nonumber\\
&amp;= \mathop {argmax}\limits_{ {\mu _1}} \sum\limits_{i = 1}^N { {y_i}\log \frac{1}{ { { {\left( {2\pi } \right)}^{ {p维/2}}}{ {\left| \Sigma  \right|}^{ {1/2}}}}}} {e^{ {\text{ - }}\frac{ {\text{1}}}{ {\text{2}}}{ {\left( { {x_i} - {\mu _1}} \right)}^T}{\Sigma^{-1}}\left( { {x_i} - {\mu _1}} \right)}}\\
&amp;= \mathop {argmax}\limits_{ {\mu _1}} \sum\limits_{i = 1}^N { {y_i}\left( { {\text{ - }}\frac{ {\text{1}}}{ {\text{2}}}{ {\left( { {x_i} - {\mu _1}} \right)}^T}{\Sigma^{-1}}\left( { {x_i} - {\mu _1}} \right)} \right)} \\
&amp;=\mathop{argmin}_{\mu_1}\sum\limits_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)
\end{align*}
\]</span> 由于： <span class="math display">\[
\begin{align*}
\sum\limits_{i=1}^Ny_i(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)&amp;= \sum\limits_{i = 1}^N { {y_i}} \left( { {x_i}^T{\Sigma ^{ - 1}} - {\mu _1}^T{\Sigma ^{ - 1}}} \right)({x_i} - {\mu _1})\\
&amp;= \sum\limits_{i = 1}^N { {y_i}} \left( { {x_i}^T{\Sigma ^{ - 1}}{x_i} - {x_i}^T{\Sigma ^{ - 1}}{\mu _1} - {\mu _1}^T{\Sigma ^{ - 1}}{x_i} + {\mu _1}^T{\Sigma ^{ - 1}}{\mu _1}} \right)\\
&amp;= \sum\limits_{i=1}^Ny_ix_i^T\Sigma^{-1}x_i-2y_i\mu_1^T\Sigma^{-1}x_i+y_i\mu_1^T\Sigma^{-1}\mu_1 \\
\end{align*}
\]</span></p></li>
</ul>
<p>对上式求<span class="math inline">\(\mu_1\)</span> 的微分，可以得到： <span class="math display">\[
\begin{align*}
&amp;\sum\limits_{i=1}^N-2y_i\Sigma^{-1}x_i+2y_i\Sigma^{-1}\mu_1=0\nonumber\\
&amp;\Longrightarrow \sum\limits_{i = 1}^N { {y_i}\left( { {\Sigma ^{ - 1}}{x_i} - {\Sigma ^{ - 1}}{\mu _1}} \right)}  = 0\\
&amp;\Longrightarrow \sum\limits_{i = 1}^N { {y_i}\left( { {x_i} - {\mu _1}} \right)}  = 0\\
&amp;\Longrightarrow\sum\limits_{i = 1}^N { {y_i}{x_i}}  = \sum\limits_{i = 1}^N { {y_i}{\mu _1}} \\
&amp;\Longrightarrow\mu_1=\frac{\sum\limits_{i=1}^Ny_ix_i}{\sum\limits_{i=1}^Ny_i}=\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1}
\end{align*}
\]</span></p>
<ul>
<li><p>求解 <span class="math inline">\(\mu_0\)</span>，由于正反例是对称的，所以： <span class="math display">\[
\mu_0=\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0}
\]</span></p></li>
<li><p>最为困难的是求解 <span class="math inline">\(\Sigma\)</span>，我们的模型假设对正反例采用相同的协方差矩阵，当然从上面的求解中我们可以看到，即使采用不同的矩阵也不会影响之前的三个参数。首先我们先看下式： <span class="math display">\[
\begin{align*}
\sum\limits_{i=1}^N\log\mathcal{N}(\mu,\Sigma)&amp;=\sum\limits_{i=1}^N\log(\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}})+(-\frac{1}{2}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\\
&amp;=\sum\limits_{i=1}^N \log \frac{1}{(2 \pi)^{p / 2}}+\log |\Sigma|^{-1 / 2}-\frac{1}{2}\left(x_{i}-\mu\right)^{T} \Sigma^{-1}\left(x_{i}-\mu\right)\\
&amp;=Const - \frac{1}{2}\sum\limits_{i = 1}^N {\log \left| \Sigma  \right|}  - \frac{1}{2}\sum\limits_{i = 1}^N { { {\left( { {x_i} - \mu } \right)}^T}{\Sigma ^{ - 1}}\left( { {x_i} - \mu } \right)} \\
&amp;\because { { {\left( { {x_i} - \mu } \right)}^T}{\Sigma ^{ - 1}}\left( { {x_i} - \mu } \right)} \ is \ a \ scalar \ \\
&amp;\therefore Trace({ { {\left( { {x_i} - \mu } \right)}^T}{\Sigma ^{ - 1}}\left( { {x_i} - \mu } \right)}) = { { {\left( { {x_i} - \mu } \right)}^T}{\Sigma ^{ - 1}}\left( { {x_i} - \mu } \right)}\\
&amp;=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}N*Trace((x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\nonumber\\
&amp;\because \ Trace(ABC)=Trace(CAB)=Trace(BCA)\\
&amp;=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}N*Trace((x_i-\mu)(x_i-\mu)^T\Sigma^{-1})\nonumber\\
&amp;=Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}N*Trace(S\Sigma^{-1})
\end{align*}
\]</span> 在这个表达式中，我们在标量上加入迹从而可以交换矩阵的顺序，对于包含绝对值和迹的表达式的导数，我们有： <span class="math display">\[
\begin{align}
\frac{\partial}{\partial A}(|A|)&amp;=|A|A^{-1}\\
\frac{\partial}{\partial A}Trace(AB)&amp;=B^T
\end{align}
\]</span> 因此： <span class="math display">\[
\begin{align}
\hat \Sigma &amp;=\mathop {argmax}\limits_{ {\Sigma}}\sum\limits_{i=1}^N((1-y_i)\log\mathcal{N}(\mu_0,\Sigma)+y_i\log \mathcal{N}(\mu_1,\Sigma) \nonumber\\
&amp;=\mathop {argmax}\limits_{ {\Sigma}}Const-\frac{1}{2}N\log|\Sigma|-\frac{1}{2}N_1Trace(S_1\Sigma^{-1})-\frac{1}{2}N_2Trace(S_2\Sigma^{-1})
\end{align}
\]</span> 其中，<span class="math inline">\(S_1,S_2\)</span> 分别为两个类数据内部的协方差矩阵，于是对上式求微分可得： <span class="math display">\[
\begin{align}
&amp;N\frac{1}{ {\left| \Sigma  \right|}}\left| \Sigma  \right|{\Sigma ^{ - 1}} + {N_1}S_1^T\left( { - 1} \right){\Sigma ^{ - 2}} + {N_2}S_2^T\left( { - 1} \right){\Sigma ^{ - 2}}=0 \\
&amp;\Longrightarrow N\Sigma^{-1}-N_1S_1^T\Sigma^{-2}-N_2S_2^T\Sigma^{-2}=0\nonumber\\
&amp;\Longrightarrow \hat \Sigma=\frac{N_1S_1+N_2S_2}{N}
\end{align}
\]</span> 这里应用了类协方差矩阵的对称性。</p></li>
</ul>
<p>于是我们就利用最大后验的方法求得了我们模型假设里面的所有参数，根据模型，可以得到联合分布，也就可以得到用于推断的条件分布了。</p>
<h2 id="软分类-朴素贝叶斯">软分类-朴素贝叶斯</h2>
<p>上面的高斯判别分析的是对数据集的分布作出了高斯分布的假设，同时引入伯努利分布作为类先验，从而利用最大后验求得这些假设中的参数。</p>
<p>而朴素贝叶斯（概率生成模型）对数据的属性之间的关系作出了假设，一般地，我们有需要得到 <span class="math inline">\(p(x|y)\)</span> 这个概率值，由于 <span class="math inline">\(x\)</span> 有 <span class="math inline">\(p\)</span> 个维度，因此需要对这么多的维度的联合概率进行采样，但是我们知道这么高维度的空间中采样需要的样本数量非常大才能获得较为准确的概率近似。</p>
<p>在一般的有向概率图模型中，对各个<strong>属性维度</strong>之间的<strong>条件独立</strong>关系作出了不同的假设，其中最为简单的一个假设就是在朴素贝叶斯模型描述中的条件独立性假设。 <span class="math display">\[
p(x|y)=\prod\limits_{i=1}^pp(x_i|y)
\]</span> 即： <span class="math display">\[
x_i\perp x_j|y,\forall\  i\ne j
\]</span> 于是利用贝叶斯定理，对于单次观测： <span class="math display">\[
p(y|x)=\frac{p(x|y)p(y)}{p(x)}=\frac{\prod\limits_{i=1}^pp(x_i|y)p(y)}{p(x)}
\]</span> 对于单个维度的条件概率以及类先验作出进一步的假设：</p>
<ol type="1">
<li><span class="math inline">\(x_i\)</span> 为连续变量：<span class="math inline">\(p(x_i|y)=\mathcal{N}(\mu_i,\sigma_i^2)\)</span></li>
<li><span class="math inline">\(x_i\)</span> 为离散变量：类别分布（Categorical）：<span class="math inline">\(p(x_i=i|y)=\theta_i,\sum\limits_{i=1}^K\theta_i=1\)</span></li>
<li><span class="math inline">\(p(y)=\phi^y(1-\phi)^{1-y}\)</span></li>
</ol>
<p>对这些参数的估计，常用 MLE 的方法直接在数据集上估计，由于不需要知道各个维度之间的关系，因此，所需数据量大大减少了。估算完这些参数，再代入贝叶斯定理中得到类别的后验分布。</p>
<h2 id="小结">小结</h2>
<p>分类任务分为两类，对于需要直接输出类别的任务，感知机算法中我们在线性模型的基础上加入符号函数作为激活函数，那么就能得到这个类别，但是符号函数不光滑，于是我们采用错误驱动的方式，引入 <span class="math inline">\(\sum\limits_{x_i\in\mathcal{D}_{wrong}}-y_iw^Tx_i\)</span> 作为损失函数，然后最小化这个误差，采用批量随机梯度下降的方法来获取最佳的参数值。而在线性判别分析中，我们将线性模型看作是数据点在某一个方向的投影，采用类内小，类间大的思路来定义损失函数，其中类内小定义为两类数据的方差之和，类间大定义为两类数据中心点的间距，对损失函数求导得到参数的方向，这个方向就是 <span class="math inline">\(S_w^{-1}(\overline x_{c1}-\overline x_{c2})\)</span>，其中 <span class="math inline">\(S_w\)</span> 为原数据集两类的方差之和。</p>
<p>另一种任务是输出分类的概率，对于概率模型，我们有两种方案，第一种是<strong>判别模型</strong>，也就是直接对类别的条件概率建模，将线性模型套入 Logistic 函数中，我们就得到了 Logistic 回归模型，这里的概率解释是两类的联合概率比值的对数是线性的，我们定义的损失函数是交叉熵（等价于 MLE），对这个函数求导得到 <span class="math inline">\(\frac{1}{N}\sum\limits_{i=1}^N(y_i-p_1)x_i\)</span>，同样利用批量随机梯度（上升）的方法进行优化。第二种是<strong>生成模型</strong>，生成模型引入了类别的先验，在高斯判别分析中，我们对数据集的数据分布作出了假设，其中类先验是二项分布，而每一类的似然是高斯分布，对这个联合分布的对数似然进行最大化就得到了参数， <span class="math inline">\(\frac{\sum\limits_{i=1}^Ny_ix_i}{N_1},\frac{\sum\limits_{i=1}^N(1-y_i)x_i}{N_0},\frac{N_1S_1+N_2S_2}{N},\frac{N_1}{N}\)</span>。在朴素贝叶斯中，我们进一步对属性的各个维度之间的依赖关系作出假设，条件独立性假设大大减少了数据量的需求。</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title>线性回归</title>
    <url>/2020/03/05/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h1 id="线性回归">线性回归</h1>
<p>假设数据集为：</p>
<p><span class="math display">\[
\mathcal{D}=\{(x_1, y_1),(x_2, y_2),\cdots,(x_N, y_N)\}
\]</span> 其中<span class="math inline">\({x_i} \in {\mathbb{R}^p},{y_i} \in \mathbb{R},i = 1,2, \cdots ,N\)</span></p>
<p>我们记：</p>
<p><span class="math display">\[
X=(x_1,x_2,\cdots,x_N)^T=\left( {\begin{array}{*{20}{c}}
{ {x_{11}}}&amp;{ {x_{12}} }&amp; \cdots &amp;{ {x_{1p}} }\\
{ {x_{21}}}&amp;{ {x_{22}} }&amp; \cdots &amp;{ {x_{2p}} }\\
 \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
{ {x_{N1}} }&amp;{ {x_{N2}} }&amp; \cdots &amp;{ {x_{Np}} }
\end{array}} \right)
\]</span></p>
<p><span class="math display">\[
Y=(y_1,y_2,\cdots,y_N)^T=\left( {\begin{array}{*{20}{c}}
{ {y_1} }\\
{ {y_2} }\\
 \vdots \\
{ {y_N} }
\end{array}} \right)
\]</span></p>
<p>线性回归假设：</p>
<p><span class="math display">\[
f(w)=w^Tx
\]</span></p>
<h2 id="最小二乘法">最小二乘法</h2>
<p>对这个问题，采用二范数定义的平方误差来定义损失函数：</p>
<p><span class="math display">\[
L(w)=\sum\limits_{i=1}^N||w^Tx_i-y_i||^2_2
\]</span> 展开得到：</p>
<p><span class="math display">\[
\begin{align}
L(w)&amp;=\sum\limits_{i=1}^N(w^Tx_i-y_i)^2 \\
&amp;=(w^Tx_1-y_1,\cdots,w^Tx_N-y_N)\cdot (w^Tx_1-y_1,\cdots,w^Tx_N-y_N)^T\nonumber\\
&amp;=(w^TX^T-Y^T)\cdot (Xw-Y)=w^TX^TXw-Y^TXw-w^TX^TY+Y^TY\nonumber\\
&amp;=w^TX^TXw-2w^TX^TY+Y^TY
\end{align}
\]</span> 最小化这个值的 $ $ ：</p>
<p><span class="math display">\[
\begin{align}
\hat{w}=\mathop{argmin}\limits_wL(w)&amp;\longrightarrow\frac{\partial}{\partial w}L(w)=0\nonumber\\
&amp;\longrightarrow2X^TX\hat{w}-2X^TY=0\nonumber\\
&amp;\longrightarrow \hat{w}=(X^TX)^{-1}X^TY=X^+Y
\end{align}
\]</span> 这个式子中 <span class="math inline">\((X^TX)^{-1}X^T\)</span> 又被称为广义逆。对于行满秩或者列满秩的 <span class="math inline">\(X\)</span>，可以直接求解，但是对于非满秩的样本集合，需要使用奇异值分解（SVD）的方法。</p>
<p>在几何上，最小二乘法相当于模型和试验值的距离的平方求和。</p>
<p>换一个角度来看，我们把<span class="math inline">\(X\)</span>看成是张成一个 <span class="math inline">\(p\)</span> 维空间（满秩的情况）：<span class="math inline">\(X=Span(x_1,\cdots,x_N)\)</span>，而模型可以写成 <span class="math inline">\(f(w)=X\beta\)</span>，也就是 <span class="math inline">\(x_1,\cdots,x_p\)</span> 的某种组合，而最小二乘法就是说希望 <span class="math inline">\(Y\)</span> 和这个模型距离越小越好，于是根据投影规则它们的差应该与这个张成的空间垂直：</p>
<p><span class="math display">\[
X^T\cdot(Y-X\beta)=0\longrightarrow\beta=(X^TX)^{-1}X^TY
\]</span></p>
<h2 id="概率角度下的线性回归">概率角度下的线性回归</h2>
<p>假设我们的样本数据中的噪声<span class="math inline">\(\epsilon\)</span> 服从高斯分布，即<span class="math inline">\(\epsilon\sim\mathcal{N}(0,\sigma^2)\)</span> 。</p>
<p><span class="math display">\[
\begin{array}{l}
\because y = f\left( \omega  \right) + \varepsilon  = {\omega ^T}x + \varepsilon \\
\therefore y|x;w \sim \mathcal{N}\left( { {\omega ^T}x,{\sigma ^2}} \right)\longrightarrow\frac{1}{ {\sqrt {2\pi } \sigma }}{e^{ - \frac{ { { {\left( {y - {\omega ^T}x} \right)}^2}}}{ {2{\sigma ^2}}}}}
\end{array}
\]</span> 代入极大似然估计中：</p>
<p><span class="math display">\[
\begin{align}
L(w)=\log p(Y|X,w)&amp;=\log\prod\limits_{i=1}^Np(y_i|x_i,w)\nonumber \\
&amp;=\sum\limits_{i = 1}^N \log(p(y_i|x_i,w))\nonumber\\
&amp;=\sum\limits_{i=1}^N\log(\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}})\nonumber\\
&amp;=\sum\limits_{i = 1}^N {\left[ {\log (\frac{1}{ {\sqrt {2\pi \sigma } }}) - \frac{ { { {({y_i} - {w^T}{x_i})}^2}}}{ {2{\sigma ^2}}}} \right]} \nonumber\\
\therefore \mathop{argmax}\limits_wL(w)&amp;=\mathop{argmin}\limits_w\sum\limits_{i=1}^N(y_i-w^Tx_i)^2
\end{align}
\]</span> 这个表达式和最小二乘估计得到的结果一样。即最小二乘估计隐含了数据的噪声服从高斯分布。</p>
<h2 id="正则化">正则化</h2>
<p>在实际应用时，如果样本容量不远远大于样本的特征维度，很可能造成过拟合，对这种情况，我们有下面三个解决方式：</p>
<ol type="1">
<li><p>加数据，数据增强</p></li>
<li><p>特征选择/特征提取。</p></li>
<li><p>正则化</p></li>
</ol>
<p>正则化一般是在损失函数（如上面介绍的最小二乘损失）上加入正则化项（表示模型的复杂度对模型的惩罚），下面我们介绍一般情况下的两种正则化框架。</p>
<p><span class="math display">\[
\begin{align}
L1&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||_1,\lambda\gt0 \nonumber\\
L2&amp;:\mathop{argmin}\limits_wL(w)+\lambda||w||^2_2,\lambda \gt 0 \nonumber
\end{align}
\]</span></p>
<p>下面对最小二乘误差分别分析这两者的区别。</p>
<h3 id="l1-lasso">L1 Lasso</h3>
<p><strong>L1</strong> 正则化相当于：</p>
<p><span class="math display">\[
\mathop{argmin}\limits_wL(w)\\
s.t. ||w||_1\lt C
\]</span></p>
<p>我们已经看到平方误差损失函数在 <span class="math inline">\(w\)</span> 空间是一个椭球，因此上式求解就是椭球和 <span class="math inline">\(||w||_1=C\)</span>的切点，因此更容易相切在坐标轴上。</p>
<p>注意<strong>L1</strong>正则化容易引起稀疏解。</p>
<h3 id="l2-ridge">L2 Ridge</h3>
<p><strong>L2</strong>正则化实践中使用的最多。对于损失函数<span class="math inline">\(J\left( w \right)\)</span> :</p>
<p><span class="math display">\[
\begin{align}
J\left( w \right) &amp;= L(w) + \lambda {w^T}w \nonumber\\
&amp;= {w^T}{X^T}Xw - 2{w^T}{X^T}Y + {Y^T}Y + \lambda {w^T}w \nonumber\\
&amp;={w^T}\left( { {X^T}X + \lambda \mathbb{I}} \right)w - 2{w^T}{X^T}Y + {Y^T}Y \nonumber
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\hat{w}=\mathop{argmin}\limits_wJ\left( w \right)&amp;\longrightarrow\frac{\partial}{\partial w}J\left( w \right)=0\nonumber\\
&amp;\longrightarrow{\rm{2}}\left( { {X^T}X + \lambda \mathbb{I}} \right)w - 2{X^T}Y = 0\nonumber\\
&amp;\longrightarrow \hat{w}=(X^TX+\lambda \mathbb{I})^{-1}X^TY \nonumber
\end{align}
\]</span></p>
<p>可以看到，利用2范数进行正则化不仅可以使模型选择 <span class="math inline">\(w\)</span> 较小的参数，同时也避免 <span class="math inline">\(X^TX\)</span> 不可逆的问题。因为半正定 <span class="math inline">\({X^T}X\)</span> 加上对角矩阵一定是正定矩阵，即一定可逆。</p>
<h2 id="贝叶斯角度下的线性回归">贝叶斯角度下的线性回归</h2>
<p>从贝叶斯角度来看，我们将<span class="math inline">\(w\)</span> 看作一个先验，取先验分布 <span class="math inline">\(w\sim\mathcal{N}(0,\sigma_0^2)\)</span>。</p>
<p>又因为：</p>
<p><span class="math display">\[
P\left( {w|y} \right) = \frac{ {P\left( {y|w} \right)P\left( w \right)}}{ {P\left( y \right)}}
\]</span></p>
<p>所以MAP最大后验估计参数<span class="math inline">\(w\)</span> :</p>
<p><span class="math display">\[
\begin{align}
\hat{w}=\mathop{argmax}\limits_wp(w|Y)&amp;=\mathop{argmax}\limits_wp(Y|w)p(w)\nonumber\\
&amp;=\mathop{argmax}\limits_w\log p(Y|w)p(w)\nonumber\\
&amp;=\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w))\nonumber\\
\end{align}
\]</span></p>
<p>又因为第二章节可知，<span class="math inline">\(y|x;w \sim \mathcal{N}\left( { {\omega ^T}x,{\sigma ^2}} \right)\longrightarrow\frac{1}{ {\sqrt {2\pi } \sigma }}{e^{ - \frac{ { { {\left( {y - {\omega ^T}x} \right)}^2}}}{ {2{\sigma ^2}}}}}\)</span> ，所以 <span class="math inline">\(P\left( {y|w} \right)=\frac{1}{ {\sqrt {2\pi } \sigma }}{e^{ - \frac{ { { {\left( {y - {\omega ^T}x} \right)}^2}}}{ {2{\sigma ^2}}}}}\)</span> 。根据先验分布，我们知道<span class="math inline">\(P\left( w \right) = \frac{1}{ {\sqrt {2\pi } {\sigma _0}}}{e^{ - \frac{ { { {\left\| w \right\|}^2}}}{ {2{\sigma _0}^2}}}}\)</span> 。</p>
<p>所以，</p>
<p><span class="math display">\[
\begin{align}
\hat{w}&amp;=\mathop{argmax}\limits_w\sum\limits_{i = 1}^N {\left[ {\log (\frac{1}{ {\sqrt {2\pi } \sigma }}\frac{1}{ {\sqrt {2\pi } {\sigma _0}}}) + \log ({e^{ - \frac{ { { { \left( { {y_i} - {w^T}{x_i}} \right)}^2}}}{ {2{\sigma ^2}}} - \frac{ { { {\left\| w \right\|}^2}}}{ {2{\sigma _0}^2}}}})} \right]} \nonumber\\
&amp;=\mathop {\arg \min }\limits_w \sum\limits_{i = 1}^N {\left[ {\frac{ { { {\left( { {y_i} - {w^T}{x_i}} \right)}^2}}}{ {2{\sigma ^2}}} + \frac{ { { {\left\| w \right\|}^2}}}{ {2{\sigma _0}^2}}} \right]} \nonumber\\
&amp;=\mathop {\arg \min }\limits_w \sum\limits_{i = 1}^N {\left[ { { {\left( { {y_i} - {w^T}{x_i}} \right)}^2} + \frac{ { {\sigma ^2}}}{ { {\sigma _0}^2}}{ {\left\| w \right\|}^2}} \right]} \nonumber
\end{align}
\]</span></p>
<p>如果我们把<span class="math inline">\({\frac{ { {\sigma ^2}}}{ { {\sigma _0}^2} } }\)</span> 记作<span class="math inline">\(\lambda\)</span> ，我们将会看到，由于超参数 <span class="math inline">\(\sigma_0\)</span>的存在和 <strong>L2</strong> Ridge 正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 <strong>L1</strong> 正则类似的结果。</p>
<h2 id="小结">小结</h2>
<p>线性回归模型是最简单的模型，但是麻雀虽小，五脏俱全，在这里，我们利用最小二乘误差得到了闭式解析解。同时也发现，在噪声为高斯分布的时候，MLE极大似然估计的解等价于最小二乘估计LSE。而增加了<strong>L2</strong>正则项后的LSE，等价于高斯噪声先验下的 MAP最大后验估计解，加上 <strong>L1</strong> 正则项后，等价于 Laplace 噪声先验。</p>
<p>传统的机器学习方法或多或少都有线性回归模型的影子：</p>
<ol type="1">
<li>线性模型往往不能很好地拟合数据，因此有三种方案克服这一劣势：
<ul>
<li><p>对特征的维数进行变换，例如多项式回归模型就是在线性特征的基础上加入高次项。</p></li>
<li><p>在线性方程后面加入一个非线性变换，即引入一个非线性的激活函数，典型的有线性分类模型如感知机。</p></li>
<li><p>对于一致的线性系数，我们进行多次变换，这样同一个特征不仅仅被单个系数影响，例如多层感知机（深度前馈网络）。</p></li>
</ul></li>
<li>线性回归在整个样本空间都是线性的，我们修改这个限制，在不同区域引入不同的线性或非线性，例如线性样条回归和决策树模型。</li>
<li>线性回归中使用了所有的样本，但是对数据预先进行加工学习的效果可能更好（所谓的维数灾难，高维度数据更难学习），例如 PCA 算法和流形学习。</li>
</ol>
]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
  <entry>
    <title>单例模式</title>
    <url>/2020/02/11/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<h2 id="单例模式的定义">单例模式的定义</h2>
<p>单例模式（Singleton Pattern）是最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p>
<p>这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。</p>
<p><strong>注意：</strong></p>
<ul>
<li>1、单例类只能有一个实例。</li>
<li>2、单例类必须自己创建自己的唯一实例。</li>
<li>3、单例类必须给所有其他对象提供这一实例。</li>
</ul>
<h2 id="方法一基类">方法一：基类</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Singleton</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(cls, <span class="string">'_instance'</span>):</span><br><span class="line">            cls._instance = super().__new__(cls, *args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> cls._instance</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(Singleton)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">foo1 = Foo()</span><br><span class="line">foo2 = Foo()</span><br><span class="line"></span><br><span class="line">print(foo1 <span class="keyword">is</span> foo2)  <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p><strong>说明</strong>：</p>
<p>方法一借助了<code>__new__</code>的特性：</p>
<ul>
<li><p><code>__new__</code>是在<code>__init__</code>之前被调用的特殊方法；</p></li>
<li><p><code>__new__</code>是用来创建对象并返回这个对象，而<code>__init__</code>只是将传入的参数初始化给对象</p></li>
</ul>
<p>实际中，很少会用到<code>__new__</code>，除非你希望能够控制对象的创建。</p>
<p>在这里，类Singleton是我们要创建的对象，我们希望能够自定义它使之完成单例模式，所以我们改写了<code>__new__</code>。因为自定义的<code>__new__</code>重载了父类的<code>__new__</code>，所以要自己显式调用父类的<code>__new__</code>，即object.<code>__new__</code>(cls, *args, **kwargs)，或者用super().<code>__new__</code>(cls, *args, **kwargs)。</p>
<h2 id="方法二-元类">方法二： 元类</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Singleton</span><span class="params">(type)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(cls, *args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(cls, <span class="string">'_instance'</span>):</span><br><span class="line">            cls._instance = super().__call__(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> cls._instance</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(metaclass=Singleton)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">foo1 = Foo()  <span class="comment"># Foo类是元类的实例，这里调用元类的__call__</span></span><br><span class="line">foo2 = Foo()</span><br><span class="line"></span><br><span class="line">print(foo1 <span class="keyword">is</span> foo2)  <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p><strong>说明</strong></p>
<p>1、 为什么构建元类的时候重载<code>__call__</code>，而不是<code>__new__</code>?</p>
<p>回答：首先说一下元类metaclass的<code>__new__</code>和普通类class的<code>__new__</code>的区别。在方法一我们提到过，普通类class中的<code>__new__</code>的作用是创建对象并返回这个对象，并且是在<code>__init__</code>之前被调用。而元类metaclass的<code>__new__</code>会在你<strong>定义类</strong>(根据上面的例子，这里是类Foo)的时候执行，并且只会执行<strong>一次</strong>。</p>
<p>说到这里，我们可以发现元类的<code>__new__</code>由于只能执行一次，而且在定义的时候就执行了，所以我们不可能依赖元类的<code>__new__</code>来完成单例模式的特性，即无论你多次创建类Foo的实例，元类的<code>__new__</code>早就在定义类Foo的时候就执行完毕了。</p>
<p>所以，我们把目光转向<code>__call__</code>。我们都知道<code>__call__</code>会在你每次实例化的时候调用。又因为类Foo是元类Singleton创建出来的类，可以认为Foo是Singleton的实例对象。所以每次Foo()的时候都会去调用元类的<code>__call__</code>。而在<code>__call__</code>中我们拿到已经创建好的实例对象，不就是单例吗。</p>
<h2 id="方法三使用模块">方法三：使用模块</h2>
<p>其实，<strong>Python 的模块就是天然的单例模式</strong>，因为模块在第一次导入时，<strong>会生成 .pyc 文件</strong>，当第二次导入时，就会直接加载 .pyc 文件，而不会再次执行模块代码。因此，我们只需把相关的函数和数据定义在一个模块中，就可以获得一个单例对象了。如果我们真的想要一个单例类，可以考虑这样做：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Singleton</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line">singleton = Singleton()</span><br></pre></td></tr></table></figure>
<p>将上面的代码保存在文件 <code>mysingleton.py</code> 中，要使用时，直接在其他文件中导入此文件中的对象，这个对象即是单例模式的对象。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mysingleton <span class="keyword">import</span> singleton</span><br></pre></td></tr></table></figure>
<h2 id="方法四-装饰器">方法四： 装饰器</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">singleton</span><span class="params">(class_)</span>:</span></span><br><span class="line">    instances = &#123;&#125; <span class="comment"># 为什么这里不直接为None,因为内部函数没法访问外部函数的非容器变量(闭包特性)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getinstance</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> class_ <span class="keyword">not</span> <span class="keyword">in</span> instances:</span><br><span class="line">            instances[class_] = class_(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> instances[class_]</span><br><span class="line">    <span class="keyword">return</span> getinstance</span><br><span class="line"></span><br><span class="line"><span class="meta">@singleton</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span><span class="params">(BaseClass)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">foo1 = Foo()</span><br><span class="line">foo2 = Foo()</span><br><span class="line"></span><br><span class="line">print(foo1 <span class="keyword">is</span> foo2)  <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Singleton</tag>
      </tags>
  </entry>
  <entry>
    <title>元类</title>
    <url>/2020/02/10/%E5%85%83%E7%B1%BB/</url>
    <content><![CDATA[<h2 id="类也是对象">类也是对象</h2>
<p>在理解<code>metaclass</code>之前，我们先要掌握python中的类<code>class</code>是什么。 python中类的概念，是借鉴自smalltalk语言。 在大部分语言中，类指的是"描述如何产生一个对象(object)"的一段代码，这对于python也是如此。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">ObjectCreator</span><span class="params">()</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>my_object = ObjectCreator()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(my_object)</span><br><span class="line">&lt;__main__.ObjectCreator object at <span class="number">0x8974f2c</span>&gt;</span><br></pre></td></tr></table></figure>
<p>但是，在python中，类远不止如此，类同时也是对象。 当你遇到关键词<code>class</code>的时候，Python就会自动执行产生一个对象。下面的代码段中:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">ObjectCreator</span><span class="params">()</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>Python在内存中产生了一个名叫做"ObjectCreator"的对象。这个对象(类)自身拥有产生对象(实例instance)的能力。 同时，它也是一个对象，因此你可以对它做如下操作:</p>
<ul>
<li><p>赋值给变量</p></li>
<li><p>复制它</p></li>
<li><p>为它增加属性(attribute)</p></li>
<li><p>作为参数传值给函数</p></li>
</ul>
<p>举例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(ObjectCreator) <span class="comment"># 你可以打印一个类,因为它同时也是对象</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">ObjectCreator</span>'&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">def</span> <span class="title">echo</span><span class="params">(o)</span>:</span></span><br><span class="line"><span class="meta">... </span>    print(o)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>echo(ObjectCreator) <span class="comment"># 作为参数传值给函数</span></span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">ObjectCreator</span>'&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">print</span><span class="params">(hasattr<span class="params">(ObjectCreator, <span class="string">'new_attribute'</span>)</span>)</span></span></span><br><span class="line"><span class="class"><span class="title">False</span></span></span><br><span class="line">&gt;&gt;&gt; ObjectCreator.new_attribute = 'foo' # you can add attributes to a class</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(hasattr(ObjectCreator, <span class="string">'new_attribute'</span>))</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(ObjectCreator.new_attribute)</span><br><span class="line">foo</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ObjectCreatorMirror = ObjectCreator <span class="comment"># 将类赋值给变量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(ObjectCreatorMirror.new_attribute)</span><br><span class="line">foo</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(ObjectCreatorMirror())</span><br><span class="line">&lt;__main__.ObjectCreator object at <span class="number">0x8997b4c</span>&gt;</span><br></pre></td></tr></table></figure>
<h2 id="type">type()</h2>
<p>先来说说大家所认识的<code>type</code>。这个古老而好用的函数，可以让我们知道一个对象的类型是什么。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(<span class="number">1</span>))</span><br><span class="line">&lt;type <span class="string">'int'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(<span class="string">"1"</span>))</span><br><span class="line">&lt;type <span class="string">'str'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(ObjectCreator))</span><br><span class="line">&lt;type <span class="string">'type'</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(ObjectCreator()))</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">ObjectCreator</span>'&gt;</span></span><br></pre></td></tr></table></figure>
<p>实际上，<code>type</code>还有一个完全不同的功能，它可以在运行时产生类。<code>type</code>可以传入一些参数，然后返回一个类。下面举例<code>type</code>创建类的用法。首先，对于类一般是这么定义的:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MyShinyClass</span><span class="params">(object)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>在下面，MyShinyClass也可以这样子被创建出来,并且跟上面的创建方法有一样的表现:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>MyShinyClass = type(<span class="string">'MyShinyClass'</span>, (), &#123;&#125;) <span class="comment"># returns a class object</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(MyShinyClass)</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">__main__</span>.<span class="title">MyShinyClass</span>'&gt;</span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">print</span><span class="params">(MyShinyClass<span class="params">()</span>)</span> # <span class="title">create</span> <span class="title">an</span> <span class="title">instance</span> <span class="title">with</span> <span class="title">the</span> <span class="title">class</span></span></span><br><span class="line"><span class="class">&lt;<span class="title">__main__</span>.<span class="title">MyShinyClass</span> <span class="title">object</span> <span class="title">at</span> 0<span class="title">x8997cec</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>type</code>创建类需要传入三个参数,分别为:</p>
<ul>
<li>类的名字</li>
<li>一组"类的父类"的元组(tuple) (这个会实现继承,也可以为空)</li>
<li>字典 (类的属性名与值,key-value的形式，不传相当于为空).</li>
</ul>
<p>下面来点复杂的，来更好的理解<code>type</code>传入的三个参数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span><span class="params">(object)</span>:</span></span><br><span class="line">    bar = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">echo_bar</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(self.bar)</span><br></pre></td></tr></table></figure>
<p>等价于:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">echo_bar</span><span class="params">(self)</span>:</span></span><br><span class="line">    print(self.bar)</span><br><span class="line"></span><br><span class="line">Foo = type(<span class="string">'Foo'</span>, (), &#123;<span class="string">'bar'</span>:<span class="literal">True</span>, <span class="string">'echo_bar'</span>: echo_bar&#125;)</span><br></pre></td></tr></table></figure>
<p>看点有继承关系的类的实现:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FooChild</span><span class="params">(Foo)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>等价于:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">FooChild = type(<span class="string">'FooChild'</span>, (Foo, ), &#123;&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="什么是元类">什么是元类</h2>
<p>metaclass 就是创建类的那家伙。(事实上，<code>type</code>就是一个metaclass)</p>
<p>我们知道,我们定义了class就是为了能够创建object的，那么，metaclass就是用来创造“类对象”的类.它是“类对象”的“类”。</p>
<p>可以这样子来理解:</p>
<figure>
<img src="/2020/02/10/%E5%85%83%E7%B1%BB/python_metaclass.png" alt><figcaption>python_metaclass</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MyClass = MetaClass()</span><br><span class="line">MyObject = MyClass()</span><br></pre></td></tr></table></figure>
<p>也可以用我们上面的<code>type</code>来表示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MyClass = type(<span class="string">'MyClass'</span>, (), &#123;&#125;)</span><br></pre></td></tr></table></figure>
<p>说白了，函数<code>type</code>就是一个特殊的metaclass。</p>
<p>python在背后使用<code>type</code>创造了所有的类。<code>type</code>是所有类的metaclass。</p>
<p>metaclass就是创造类对象的工具.如果你喜欢，你也可以称之为"类的工厂".</p>
<p>type是python內置的metaclass。不过，你也可以编写自己的metaclass.</p>
<h2 id="自定义metaclass">自定义metaclass</h2>
<p>使用metaclass的主要目的，是为了能够在创建类的时候，自动地修改类。</p>
<p>一个很傻的需求，我们决定要将该模块中的所有类的属性，改为大写。</p>
<p>有几种方法可以做到，这里使用<code>__metaclass__</code>来实现.</p>
<p>在模块的层次定义metaclass，模块中的所有类都会使用它来创造类。我们只需要告诉metaclass，将所有的属性转化为大写。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpperAttrMetaclass</span><span class="params">(type)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, clsname, bases, attrs)</span>:</span>  <span class="comment">#也可以写成__new__(cls, *args, **kwargs)</span></span><br><span class="line">        uppercase_attr = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> name, val <span class="keyword">in</span> attrs.items():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">'__'</span>):</span><br><span class="line">                uppercase_attr[name.upper()] = val</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                uppercase_attr[name] = val</span><br><span class="line">        <span class="keyword">return</span> super().__new__(cls, clsname, bases, uppercase_attr)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">UpperFcn</span><span class="params">(metaclass=UpperAttrMetaclass)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>metaclass</tag>
      </tags>
  </entry>
  <entry>
    <title>Pyhton内省(introspection)</title>
    <url>/2020/01/02/Pyhton%E5%86%85%E7%9C%81-introspection/</url>
    <content><![CDATA[<h2 id="内省">内省</h2>
<p>​ 在计算机科学中，内省指一种能力，可以确定对象是什么，包含何种信息，可以做什么。在runtime获得一个对象的全部类型信息。代码内省用于检查类、方法、对象、模块、关键字，并获取有关它们的信息，以便我们可以利用它。通过使用自省，我们可以动态地检查Python对象。</p>
<h3 id="dir">dir</h3>
<p><code>dir</code>返回属于对象的属性和方法的列表。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_list = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">dir(my_list)</span><br><span class="line"><span class="comment"># Output: ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__',</span></span><br><span class="line"><span class="comment"># '__delslice__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',</span></span><br><span class="line"><span class="comment"># '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__',</span></span><br><span class="line"><span class="comment"># '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__',</span></span><br><span class="line"><span class="comment"># '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__',</span></span><br><span class="line"><span class="comment"># '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__',</span></span><br><span class="line"><span class="comment"># '__subclasshook__', 'append', 'count', 'extend', 'index', 'insert', 'pop',</span></span><br><span class="line"><span class="comment"># 'remove', 'reverse', 'sort']</span></span><br></pre></td></tr></table></figure>
<p>当无法回忆起方法名时，这将非常方便。如果我们不带任何参数运行<code>dir()</code>，那么它将返回current scope内的所有名称。</p>
<h3 id="type-id">type &amp; id</h3>
<p><code>type</code>函数返回对象的类型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(type(<span class="string">''</span>))</span><br><span class="line"><span class="comment"># Output: &lt;type 'str'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type([]))</span><br><span class="line"><span class="comment"># Output: &lt;type 'list'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(&#123;&#125;))</span><br><span class="line"><span class="comment"># Output: &lt;type 'dict'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(dict))</span><br><span class="line"><span class="comment"># Output: &lt;type 'type'&gt;</span></span><br><span class="line"></span><br><span class="line">print(type(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># Output: &lt;type 'int'&gt;</span></span><br></pre></td></tr></table></figure>
<p><code>id</code>返回对象的唯一id值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">name = <span class="string">"Yasoob"</span></span><br><span class="line">print(id(name))</span><br><span class="line"><span class="comment"># Output: 139972439030304</span></span><br></pre></td></tr></table></figure>
<h3 id="methods-for-code-introspection">Methods for Code Introspection</h3>
<table>
<colgroup>
<col style="width: 21%">
<col style="width: 78%">
</colgroup>
<thead>
<tr class="header">
<th>Function</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>help()</strong></td>
<td>It is used it to find what other functions do</td>
</tr>
<tr class="even">
<td><strong>hasattr()</strong></td>
<td>Checks if an object has an attribute</td>
</tr>
<tr class="odd">
<td><strong>getattr()</strong></td>
<td>Returns the contents of an attribute if there are some.</td>
</tr>
<tr class="even">
<td><strong>repr()</strong></td>
<td>Return string representation of object</td>
</tr>
<tr class="odd">
<td><strong>callable()</strong></td>
<td>Checks if an object is a callable object (a function)or not.</td>
</tr>
<tr class="even">
<td><strong>issubclass()</strong></td>
<td>Checks if a specific class is a derived class of another class.</td>
</tr>
<tr class="odd">
<td><strong>isinstance()</strong></td>
<td>Checks if an objects is an instance of a specific class.</td>
</tr>
<tr class="even">
<td><strong>sys()</strong></td>
<td>Give access to system specific variables and functions</td>
</tr>
<tr class="odd">
<td><strong><strong>doc</strong></strong></td>
<td>Return some documentation about an object</td>
</tr>
<tr class="even">
<td><strong><strong>name</strong></strong></td>
<td>Return the name of the object.</td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>introspection</tag>
      </tags>
  </entry>
</search>
